INFO 04-07 14:14:20 [__init__.py:239] Automatically detected platform cuda.
==PROF== Connected to process 6899 (/usr/bin/python3.10)
==PROF== Profiling "distribution_elementwise_grid..." - 0: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 1: 0%....50%....100% - 40 passes
==PROF== Profiling "distribution_elementwise_grid..." - 2: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 3: 0%....50%....100% - 40 passes
==PROF== Profiling "distribution_elementwise_grid..." - 4: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 5: 0%....50%....100% - 40 passes
==PROF== Profiling "distribution_elementwise_grid..." - 6: 0%....50%....100% - 40 passes
==PROF== Profiling "softmax_warp_forward" - 7: 0%....50%....100% - 39 passes
==PROF== Profiling "gatherTopK" - 8: 0%....50%....100% - 40 passes
==PROF== Profiling "bitonicSortKVInPlace" - 9: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 10: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 11: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 12: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 13: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 14: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 15: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 16: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 17: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 18: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 19: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 20: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 21: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 22: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 23: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 24: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 25: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 26: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 27: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 28: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 29: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 30: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 31: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 32: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 33: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 34: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 35: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 36: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 37: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 38: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 39: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 40: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 41: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 42: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 43: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 44: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 45: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 46: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 47: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 48: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 49: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 50: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 51: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 52: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 53: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 54: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 55: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 56: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 57: 0%....50%....100% - 40 passes
==PROF== Profiling "vectorized_elementwise_kernel" - 58: 0%....50%....100% - 40 passes
==PROF== Profiling "moe_align_block_size_kernel" - 59: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 60: 0%....50%....100% - 40 passes
==PROF== Profiling "act_and_mul_kernel" - 61: 0%....50%....100% - 40 passes
==PROF== Profiling "fused_moe_kernel" - 62: 0%....50%....100% - 40 passes
==PROF== Profiling "reduce_kernel" - 63: 0%....50%....100% - 40 passes
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
{'block_m': 64, 'block_n': 32, 'block_k': 128}
fused-moe-blocksize-performance:
       m  CM block_m=64 block_n=32 block_k=128
0  256.0                              0.005485
==PROF== Disconnected from process 6899
[6899] python3.10@127.0.0.1
  void at::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::<unnamed>::distribution_nullary_kernel<c10::Half, float, float4, at::CUDAGeneratorImpl *, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4) (480, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.13
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle        17005
    Memory Throughput                 %        14.57
    DRAM Throughput                   %         0.19
    Duration                         us         7.52
    L1/TEX Cache Throughput           %         9.69
    L2 Cache Throughput               %        14.57
    SM Active Cycles              cycle     14366.75
    Compute (SM) Throughput           %        57.99
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 9%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         2.70
    Executed Ipc Elapsed  inst/cycle         2.29
    Issue Slots Busy               %        68.39
    Issued Ipc Active     inst/cycle         2.74
    SM Busy                        %        68.39
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (55.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         1.36
    Mem Busy                               %         7.50
    Max Bandwidth                          %        14.57
    L1/TEX Hit Rate                        %        45.02
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        99.42
    Mem Pipes Busy                         %         7.67
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        68.51
    Issued Warp Per Scheduler                        0.69
    No Eligible                            %        31.49
    Active Warps Per Scheduler          warp         8.21
    Eligible Warps Per Scheduler        warp         3.25
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.99
    Warp Cycles Per Executed Instruction           cycle        12.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.80
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 31.13%                                                                                    
          On average, each warp of this workload spends 3.7 cycles being stalled waiting for the micro scheduler to     
          select the warp to issue. Not selected warps are eligible warps that were not picked by the scheduler to      
          issue that cycle as another warp was selected. A high number of not selected warps typically means you have   
          sufficient warps to cover warp latencies and you may consider reducing the number of active warps to          
          possibly increase cache coherence and data locality. This stall type represents about 31.1% of the total      
          average of 12.0 cycles between issuing two instructions.                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         9700
    Executed Instructions                           inst      3104000
    Avg. Issued Instructions Per Scheduler          inst      9825.09
    Issued Instructions                             inst      3144028
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 6.551%                                                                                          
          This kernel executes 409088 fused and 219136 non-fused FP32 instructions. By converting pairs of non-fused    
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 17% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    480
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          122880
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        69.15
    Achieved Active Warps Per SM           warp        33.19
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 30.85%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (69.1%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          160
    Total DRAM Elapsed Cycles        cycle       669696
    Average L1 Active Cycles         cycle     14366.75
    Total L1 Elapsed Cycles          cycle      1355356
    Average L2 Active Cycles         cycle      6590.97
    Total L2 Elapsed Cycles          cycle       463584
    Average SM Active Cycles         cycle     14366.75
    Total SM Elapsed Cycles          cycle      1355356
    Average SMSP Active Cycles       cycle     14340.27
    Total SMSP Elapsed Cycles        cycle      5421424
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.06
    Branch Instructions              inst       192000
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::BUnaryFunctor<c10::Half, c10::Half, c10::Half, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.11
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle        12079
    Memory Throughput                 %        55.64
    DRAM Throughput                   %        55.64
    Duration                         us         5.31
    L1/TEX Cache Throughput           %        15.30
    L2 Cache Throughput               %        22.96
    SM Active Cycles              cycle      9151.21
    Compute (SM) Throughput           %         6.36
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.29
    Executed Ipc Elapsed  inst/cycle         0.22
    Issue Slots Busy               %         7.71
    Issued Ipc Active     inst/cycle         0.31
    SM Busy                        %         8.39
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 94.68%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       395.59
    Mem Busy                               %        20.35
    Max Bandwidth                          %        55.64
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.86
    Mem Pipes Busy                         %         5.09
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.63
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.37
    Active Warps Per Scheduler          warp         8.44
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 44.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.44 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        97.82
    Warp Cycles Per Executed Instruction           cycle       103.72
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.36%                                                                                          
          On average, each warp of this workload spends 83.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 85.6% of the total average of 97.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       665.60
    Executed Instructions                           inst       212992
    Avg. Issued Instructions Per Scheduler          inst       705.78
    Issued Instructions                             inst       225850
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.658%                                                                                          
          This kernel executes 0 fused and 32768 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.13
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 128 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        65.44
    Achieved Active Warps Per SM           warp        31.41
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 34.56%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (65.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        32834
    Total DRAM Elapsed Cycles        cycle       472064
    Average L1 Active Cycles         cycle      9151.21
    Total L1 Elapsed Cycles          cycle       965940
    Average L2 Active Cycles         cycle      6586.12
    Total L2 Elapsed Cycles          cycle       328448
    Average SM Active Cycles         cycle      9151.21
    Total SM Elapsed Cycles          cycle       965940
    Average SMSP Active Cycles       cycle      8182.78
    Total SMSP Elapsed Cycles        cycle      3863760
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.102%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 7.53% above the average, while the minimum instance value is 8.11% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.08
    Branch Instructions              inst        16384
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::<unnamed>::distribution_nullary_kernel<c10::Half, float, float4, at::CUDAGeneratorImpl *, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4) (480, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      3548023
    Memory Throughput                 %        81.65
    DRAM Throughput                   %        81.65
    Duration                         ms         1.55
    L1/TEX Cache Throughput           %        20.69
    L2 Cache Throughput               %        30.45
    SM Active Cycles              cycle   3543336.09
    Compute (SM) Throughput           %        75.06
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 14% 
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        50.33
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         3.01
    Executed Ipc Elapsed  inst/cycle         3.00
    Issue Slots Busy               %        75.16
    Issued Ipc Active     inst/cycle         3.01
    SM Busy                        %        75.16
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (59.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       587.37
    Mem Busy                               %        15.22
    Max Bandwidth                          %        81.65
    L1/TEX Hit Rate                        %        45.15
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %       100.00
    Mem Pipes Busy                         %        10.36
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        75.17
    Issued Warp Per Scheduler                        0.75
    No Eligible                            %        24.83
    Active Warps Per Scheduler          warp         8.10
    Eligible Warps Per Scheduler        warp         3.42
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        10.77
    Warp Cycles Per Executed Instruction           cycle        10.77
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.95%                                                                                    
          On average, each warp of this workload spends 3.6 cycles being stalled waiting for the micro scheduler to     
          select the warp to issue. Not selected warps are eligible warps that were not picked by the scheduler to      
          issue that cycle as another warp was selected. A high number of not selected warps typically means you have   
          sufficient warps to cover warp latencies and you may consider reducing the number of active warps to          
          possibly increase cache coherence and data locality. This stall type represents about 33.0% of the total      
          average of 10.8 cycles between issuing two instructions.                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   2663020.45
    Executed Instructions                           inst    852166544
    Avg. Issued Instructions Per Scheduler          inst   2663145.28
    Issued Instructions                             inst    852206490
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.462%                                                                                          
          This kernel executes 132157184 fused and 73420288 non-fused FP32 instructions. By converting pairs of         
          non-fused instructions to their fused                                                                         
          (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the     
          achieved FP32 performance could be increased by up to 18% (relative to its current performance). Check the    
          Source page to identify where this kernel executes FP32 instructions.                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    480
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          122880
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        67.48
    Achieved Active Warps Per SM           warp        32.39
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.52%                                                                                    
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (67.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14189692
    Total DRAM Elapsed Cycles        cycle    139026432
    Average L1 Active Cycles         cycle   3543336.09
    Total L1 Elapsed Cycles          cycle    283829960
    Average L2 Active Cycles         cycle   2962937.53
    Total L2 Elapsed Cycles          cycle     96470208
    Average SM Active Cycles         cycle   3543336.09
    Total SM Elapsed Cycles          cycle    283829960
    Average SMSP Active Cycles       cycle   3542891.30
    Total SMSP Elapsed Cycles        cycle   1135319840
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst     58744380
    Branch Efficiency                   %       100.00
    Avg. Divergent Branches                       0.04
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::BUnaryFunctor<c10::Half, c10::Half, c10::Half, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (917504, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      6668510
    Memory Throughput                 %        88.30
    DRAM Throughput                   %        88.30
    Duration                         ms         2.91
    L1/TEX Cache Throughput           %        12.60
    L2 Cache Throughput               %        18.67
    SM Active Cycles              cycle   6660359.47
    Compute (SM) Throughput           %         5.16
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        31.85
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            2
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.18
    Executed Ipc Elapsed  inst/cycle         0.18
    Issue Slots Busy               %         4.48
    Issued Ipc Active     inst/cycle         0.18
    SM Busy                        %         5.17
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 96.73%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       635.23
    Mem Busy                               %        16.59
    Max Bandwidth                          %        88.30
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.02
    Mem Pipes Busy                         %         4.13
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         4.48
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        95.52
    Active Warps Per Scheduler          warp        10.73
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.7%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 22.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.73 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       239.41
    Warp Cycles Per Executed Instruction           cycle       239.45
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.7%                                                                                           
          On average, each warp of this workload spends 195.1 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 81.5% of the total average of 239.4 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    298188.80
    Executed Instructions                           inst     95420416
    Avg. Issued Instructions Per Scheduler          inst    298228.45
    Issued Instructions                             inst     95433105
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 1.636%                                                                                          
          This kernel executes 0 fused and 14680064 non-fused FP32 instructions. By converting pairs of non-fused       
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 917504
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread       117440512
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              955.73
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        89.50
    Achieved Active Warps Per SM           warp        42.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.5%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     28840668
    Total DRAM Elapsed Cycles        cycle    261295104
    Average L1 Active Cycles         cycle   6660359.47
    Total L1 Elapsed Cycles          cycle    533480052
    Average L2 Active Cycles         cycle   5535119.91
    Total L2 Elapsed Cycles          cycle    181313632
    Average SM Active Cycles         cycle   6660359.47
    Total SM Elapsed Cycles          cycle    533480052
    Average SMSP Active Cycles       cycle   6654545.37
    Total SMSP Elapsed Cycles        cycle   2133920208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.08
    Branch Instructions              inst      7340032
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::<unnamed>::distribution_nullary_kernel<c10::Half, float, float4, at::CUDAGeneratorImpl *, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4) (480, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      1777401
    Memory Throughput                 %        78.66
    DRAM Throughput                   %        78.66
    Duration                         us       774.66
    L1/TEX Cache Throughput           %        20.65
    L2 Cache Throughput               %        30.39
    SM Active Cycles              cycle   1773233.48
    Compute (SM) Throughput           %        74.97
    ----------------------- ----------- ------------

    INF   Compute and Memory are well-balanced: To reduce runtime, both computation and memory traffic must be reduced. 
          Check both the Compute Workload Analysis and Memory Workload Analysis sections.                               

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 14% 
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        50.33
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         3.01
    Executed Ipc Elapsed  inst/cycle         3.00
    Issue Slots Busy               %        75.14
    Issued Ipc Active     inst/cycle         3.01
    SM Busy                        %        75.14
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (59.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       565.87
    Mem Busy                               %        15.20
    Max Bandwidth                          %        78.66
    L1/TEX Hit Rate                        %        45.17
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %       100.00
    Mem Pipes Busy                         %        10.34
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        75.14
    Issued Warp Per Scheduler                        0.75
    No Eligible                            %        24.86
    Active Warps Per Scheduler          warp         8.10
    Eligible Warps Per Scheduler        warp         3.44
    ---------------------------- ----------- ------------

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        10.78
    Warp Cycles Per Executed Instruction           cycle        10.78
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.48
    ---------------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.18%                                                                                    
          On average, each warp of this workload spends 3.6 cycles being stalled waiting for the micro scheduler to     
          select the warp to issue. Not selected warps are eligible warps that were not picked by the scheduler to      
          issue that cycle as another warp was selected. A high number of not selected warps typically means you have   
          sufficient warps to cover warp latencies and you may consider reducing the number of active warps to          
          possibly increase cache coherence and data locality. This stall type represents about 33.2% of the total      
          average of 10.8 cycles between issuing two instructions.                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst   1332276.07
    Executed Instructions                           inst    426328344
    Avg. Issued Instructions Per Scheduler          inst   1332400.87
    Issued Instructions                             inst    426368279
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.46%                                                                                           
          This kernel executes 66078720 fused and 36710400 non-fused FP32 instructions. By converting pairs of          
          non-fused instructions to their fused                                                                         
          (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point), higher-throughput equivalent, the     
          achieved FP32 performance could be increased by up to 18% (relative to its current performance). Check the    
          Source page to identify where this kernel executes FP32 instructions.                                         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    480
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          122880
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                   1
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        67.50
    Achieved Active Warps Per SM           warp        32.40
    ------------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.5%                                                                                     
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (67.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      6849318
    Total DRAM Elapsed Cycles        cycle     69656576
    Average L1 Active Cycles         cycle   1773233.48
    Total L1 Elapsed Cycles          cycle    142174336
    Average L2 Active Cycles         cycle   1478439.41
    Total L2 Elapsed Cycles          cycle     48329472
    Average SM Active Cycles         cycle   1773233.48
    Total SM Elapsed Cycles          cycle    142174336
    Average SMSP Active Cycles       cycle   1773286.16
    Total SMSP Elapsed Cycles        cycle    568697344
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst     29376010
    Branch Efficiency                   %       100.00
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::BUnaryFunctor<c10::Half, c10::Half, c10::Half, binary_internal::MulFunctor<float>>, std::array<char *, 2>>(int, T2, T3) (458752, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle      3266966
    Memory Throughput                 %        88.53
    DRAM Throughput                   %        88.53
    Duration                         ms         1.42
    L1/TEX Cache Throughput           %        12.85
    L2 Cache Throughput               %        19.03
    SM Active Cycles              cycle   3258073.54
    Compute (SM) Throughput           %         5.27
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        50.33
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.18
    Executed Ipc Elapsed  inst/cycle         0.18
    Issue Slots Busy               %         4.58
    Issued Ipc Active     inst/cycle         0.18
    SM Busy                        %         5.28
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 96.66%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       636.90
    Mem Busy                               %        17.85
    Max Bandwidth                          %        88.53
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        50.01
    Mem Pipes Busy                         %         4.21
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         4.60
    Issued Warp Per Scheduler                        0.05
    No Eligible                            %        95.40
    Active Warps Per Scheduler          warp        10.75
    Eligible Warps Per Scheduler        warp         0.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 11.47%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 21.7 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.75 active warps per scheduler, but only an average of 0.05 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       233.48
    Warp Cycles Per Executed Instruction           cycle       233.55
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 11.47%                                                                                          
          On average, each warp of this workload spends 190.8 cycles being stalled waiting for a scoreboard dependency  
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 81.7% of the total average of 233.5 cycles       
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst    149094.40
    Executed Instructions                           inst     47710208
    Avg. Issued Instructions Per Scheduler          inst    149133.92
    Issued Instructions                             inst     47722855
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 1.672%                                                                                          
          This kernel executes 0 fused and 7340032 non-fused FP32 instructions. By converting pairs of non-fused        
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 458752
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        58720256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              477.87
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        89.40
    Achieved Active Warps Per SM           warp        42.91
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 10.6%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (89.4%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle     14166498
    Total DRAM Elapsed Cycles        cycle    128009216
    Average L1 Active Cycles         cycle   3258073.54
    Total L1 Elapsed Cycles          cycle    261356608
    Average L2 Active Cycles         cycle   2724270.44
    Total L2 Elapsed Cycles          cycle     88827296
    Average SM Active Cycles         cycle   3258073.54
    Total SM Elapsed Cycles          cycle    261356608
    Average SMSP Active Cycles       cycle   3240498.05
    Total SMSP Elapsed Cycles        cycle   1045426432
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.08
    Branch Instructions              inst      3670016
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::<unnamed>::distribution_elementwise_grid_stride_kernel<float, 4, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void at::<unnamed>::distribution_nullary_kernel<c10::Half, float, float4, at::CUDAGeneratorImpl *, void templates::normal_and_transform<c10::Half, float, at::CUDAGeneratorImpl *, void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T3, T4)::[lambda(curandStatePhilox4_32_10 *) (instance 2)], void templates::normal_kernel<at::CUDAGeneratorImpl *>(const at::TensorBase &, double, double, T1)::[lambda() (instance 1)]::operator ()() const::[lambda() (instance 3)]::operator ()() const::[lambda(float) (instance 1)]>(at::TensorIteratorBase &, T4, const T5 &, T6)::[lambda(int, float) (instance 1)]>(long, at::PhiloxCudaState, T3, T4) (8, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        10.99
    SM Frequency                    Ghz         2.24
    Elapsed Cycles                cycle         4809
    Memory Throughput                 %         1.06
    DRAM Throughput                   %         0.66
    Duration                         us         2.14
    L1/TEX Cache Throughput           %         4.88
    L2 Cache Throughput               %         1.06
    SM Active Cycles              cycle       247.71
    Compute (SM) Throughput           %         1.51
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.13
    Executed Ipc Elapsed  inst/cycle         0.06
    Issue Slots Busy               %        29.38
    Issued Ipc Active     inst/cycle         1.18
    SM Busy                        %        29.38
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (24.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         4.66
    Mem Busy                               %         1.06
    Max Bandwidth                          %         0.93
    L1/TEX Hit Rate                        %        48.44
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.72
    Mem Pipes Busy                         %         0.20
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        29.80
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        70.20
    Active Warps Per Scheduler          warp         2.00
    Eligible Warps Per Scheduler        warp         0.37
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 70.2%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 2.00 active warps per scheduler, but only an average of 0.37 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         6.70
    Warp Cycles Per Executed Instruction           cycle         6.99
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    28.88
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 37.49%                                                                                          
          On average, each warp of this workload spends 2.5 cycles being stalled waiting for an immediate constant      
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 37.5% of the total average of 6.7 cycles between issuing two instructions.        
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst        69.80
    Executed Instructions                           inst        22336
    Avg. Issued Instructions Per Scheduler          inst        72.77
    Issued Instructions                             inst        23286
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 2.66%                                                                                           
          This kernel executes 2240 fused and 1152 non-fused FP32 instructions. By converting pairs of non-fused        
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 17% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   256
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              40
    Shared Memory Configuration Size           Kbyte           16.38
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread            2048
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.02
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            6
    Block Limit Shared Mem                block           16
    Block Limit Warps                     block            6
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        16.66
    Achieved Active Warps Per SM           warp         7.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 70.2%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (16.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          156
    Total DRAM Elapsed Cycles        cycle       188416
    Average L1 Active Cycles         cycle       247.71
    Total L1 Elapsed Cycles          cycle       384540
    Average L2 Active Cycles         cycle       232.81
    Total L2 Elapsed Cycles          cycle       130560
    Average SM Active Cycles         cycle       247.71
    Total SM Elapsed Cycles          cycle       384540
    Average SMSP Active Cycles       cycle       244.22
    Total SMSP Elapsed Cycles        cycle      1538160
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.05
    Branch Instructions              inst         1152
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void <unnamed>::softmax_warp_forward<c10::Half, c10::Half, float, 3, 0, 0>(T2 *, const T1 *, int, int, int, const bool *, int, bool) (8, 1, 1)x(8, 16, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.01
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle         5262
    Memory Throughput                 %         0.77
    DRAM Throughput                   %         0.65
    Duration                         us         2.34
    L1/TEX Cache Throughput           %         4.98
    L2 Cache Throughput               %         0.77
    SM Active Cycles              cycle       289.29
    Compute (SM) Throughput           %         0.27
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.18
    Executed Ipc Elapsed  inst/cycle         0.01
    Issue Slots Busy               %         4.84
    Issued Ipc Active     inst/cycle         0.19
    SM Busy                        %         4.84
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.23%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         4.55
    Mem Busy                               %         0.77
    Max Bandwidth                          %         0.65
    L1/TEX Hit Rate                        %           50
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        84.11
    Mem Pipes Busy                         %         0.27
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1937%                                                                                         
          The memory access pattern for global loads from L2 might not be optimal. On average, only 16.0 of the 32      
          bytes transmitted per sector are utilized by each thread. This applies to the 50.0% of sectors missed in      
          L1TEX. This could possibly be caused by a stride between threads. Check the Source Counters section for       
          uncoalesced global loads.                                                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1937%                                                                                         
          The memory access pattern for global stores to L2 might not be optimal. On average, only 16.0 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 50.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global stores.                                                                                                

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         6.34
    Issued Warp Per Scheduler                        0.06
    No Eligible                            %        93.66
    Active Warps Per Scheduler          warp         1.03
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 93.66%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 15.8 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.03 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        16.28
    Warp Cycles Per Executed Instruction           cycle        17.14
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.56
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 38.22%                                                                                          
          On average, each warp of this workload spends 6.2 cycles being stalled waiting for an immediate constant      
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 38.2% of the total average of 16.3 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst        13.30
    Executed Instructions                           inst         4256
    Avg. Issued Instructions Per Scheduler          inst           14
    Issued Instructions                             inst         4480
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.3478%                                                                                         
          This kernel executes 576 fused and 448 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 22% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      8
    Registers Per Thread             register/thread              21
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread            1024
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.01
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 90%                                                                                             
          The grid for this launch is configured to execute only 8 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           21
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         6.35
    Achieved Active Warps Per SM           warp         3.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 93.65%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (6.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          166
    Total DRAM Elapsed Cycles        cycle       205824
    Average L1 Active Cycles         cycle       289.29
    Total L1 Elapsed Cycles          cycle       420624
    Average L2 Active Cycles         cycle       359.09
    Total L2 Elapsed Cycles          cycle       142976
    Average SM Active Cycles         cycle       289.29
    Total SM Elapsed Cycles          cycle       420624
    Average SMSP Active Cycles       cycle       220.93
    Total SMSP Elapsed Cycles        cycle      1682496
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.413%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 67.36% above the average, while the minimum instance value is 88.30% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst          512
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.019%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 256 excessive sectors (50% of the total   
          512 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The   
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void sbtopk::gatherTopK<c10::Half, unsigned int, 2, 0>(detail::TensorInfo<const T1, T2>, T2, T2, bool, T2, T2, detail::TensorInfo<T1, T2>, T2, detail::TensorInfo<long, T2>, T2, T1 *) (256, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.09
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle        10842
    Memory Throughput                 %         3.76
    DRAM Throughput                   %         0.69
    Duration                         us         4.77
    L1/TEX Cache Throughput           %         5.63
    L2 Cache Throughput               %         1.26
    SM Active Cycles              cycle      7245.81
    Compute (SM) Throughput           %         6.70
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.38
    Executed Ipc Elapsed  inst/cycle         0.25
    Issue Slots Busy               %        10.03
    Issued Ipc Active     inst/cycle         0.40
    SM Busy                        %        10.03
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.29%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         4.89
    Mem Busy                               %         3.76
    Max Bandwidth                          %         3.44
    L1/TEX Hit Rate                        %        77.96
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        87.99
    Mem Pipes Busy                         %         3.44
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 1.881%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 16.0 of the 32   
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 3.175%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 5.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.87
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.13
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 86.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.2 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle         7.22
    Warp Cycles Per Executed Instruction           cycle         7.64
    Avg. Active Threads Per Warp                                21.71
    Avg. Not Predicated Off Threads Per Warp                    19.29
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 31.76%                                                                                          
          On average, each warp of this workload spends 2.3 cycles being stalled waiting on a fixed latency execution   
          dependency. Typically, this stall reason should be very low and only shows up as a top contributor in         
          already highly optimized kernels. Try to hide the corresponding instruction latencies by increasing the       
          number of active warps, restructuring the code or unrolling loops. Furthermore, consider switching to         
          lower-latency instructions, e.g. by making use of fast math compiler options. This stall type represents      
          about 31.8% of the total average of 7.2 cycles between issuing two instructions.                              
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 2.661%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 21.7 threads being active per cycle. This is further reduced  
          to 19.3 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       686.48
    Executed Instructions                           inst       219672
    Avg. Issued Instructions Per Scheduler          inst       726.41
    Issued Instructions                             inst       232451
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              41
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block             128
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread            8192
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.13
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           40
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         6.05
    Achieved Active Warps Per SM           warp         2.90
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 86.13%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (6.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          364
    Total DRAM Elapsed Cycles        cycle       422912
    Average L1 Active Cycles         cycle      7245.81
    Total L1 Elapsed Cycles          cycle       867116
    Average L2 Active Cycles         cycle       724.91
    Total L2 Elapsed Cycles          cycle       294496
    Average SM Active Cycles         cycle      7245.81
    Total SM Elapsed Cycles          cycle       867116
    Average SMSP Active Cycles       cycle      5238.59
    Total SMSP Elapsed Cycles        cycle      3468464
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 8.165%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 12.21% above the average, while the minimum instance value is 11.45% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 15.92%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 32.94% above the average, while the minimum instance value is 100.00% below the average.    
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.165%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 12.21% above the average, while the minimum instance value is 11.45% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.137%                                                                                          
          One or more L2 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 65.22% above the average, while the minimum instance value is 94.21% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.13
    Branch Instructions              inst        29441
    Branch Efficiency                   %        78.38
    Avg. Divergent Branches                      12.29
    ------------------------- ----------- ------------

  void at::bitonicSortKVInPlace<2, (int)-1, 16, 16, c10::Half, long, at::GTOp<c10::Half, 1>, unsigned int>(detail::TensorInfo<T5, T8>, T8, T8, T8, detail::TensorInfo<T6, T8>, T8, T7) (256, 1, 1)x(16, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.10
    SM Frequency                    Ghz         2.27
    Elapsed Cycles                cycle        12564
    Memory Throughput                 %         8.66
    DRAM Throughput                   %         0.59
    Duration                         us         5.54
    L1/TEX Cache Throughput           %        10.98
    L2 Cache Throughput               %         0.82
    SM Active Cycles              cycle      9907.80
    Compute (SM) Throughput           %         8.66
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.2 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.23
    Executed Ipc Elapsed  inst/cycle         0.18
    Issue Slots Busy               %         6.14
    Issued Ipc Active     inst/cycle         0.25
    SM Busy                        %         6.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.24%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         4.21
    Mem Busy                               %         6.01
    Max Bandwidth                          %         8.66
    L1/TEX Hit Rate                        %        57.42
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        86.68
    Mem Pipes Busy                         %         8.66
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 8.482%                                                                                          
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 0.7 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.482%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 0.7 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.40
    Issued Warp Per Scheduler                        0.08
    No Eligible                            %        91.60
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.08
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.34%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.87
    Warp Cycles Per Executed Instruction           cycle        12.51
    Avg. Active Threads Per Warp                                13.35
    Avg. Not Predicated Off Threads Per Warp                    12.14
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.375%                                                                                          
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 13.3 threads being active per cycle. This is further reduced  
          to 12.1 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst       576.80
    Executed Instructions                           inst       184576
    Avg. Issued Instructions Per Scheduler          inst          608
    Issued Instructions                             inst       194560
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    16
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    256
    Registers Per Thread             register/thread              60
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block       Kbyte/block            5.63
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread            4096
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.21
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 50%                                                                                             
          Threads are executed in groups of 32 threads called warps. This kernel launch is configured to execute 16     
          threads per block. Consequently, some threads in a warp are masked off and those hardware resources are       
          unused. Try changing the number of threads per block to be a multiple of 32 threads. Between 128 and 256      
          threads per block is a good initial range for experimentation. Use smaller thread blocks rather than one      
          large thread block per multiprocessor if latency affects performance.  This is particularly beneficial to     
          kernels that frequently call __syncthreads(). See the Hardware Model                                          
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more      
          details on launch configurations.                                                                             

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           15
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           15
    Theoretical Occupancy                     %        31.25
    Achieved Occupancy                        %         6.08
    Achieved Active Warps Per SM           warp         2.92
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 80.55%                                                                                          
          The difference between calculated theoretical (31.2%) and measured achieved occupancy (6.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 68.75%                                                                                          
          The 3.75 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (31.2%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          364
    Total DRAM Elapsed Cycles        cycle       491520
    Average L1 Active Cycles         cycle      9907.80
    Total L1 Elapsed Cycles          cycle      1004832
    Average L2 Active Cycles         cycle       583.84
    Total L2 Elapsed Cycles          cycle       341280
    Average SM Active Cycles         cycle      9907.80
    Total SM Elapsed Cycles          cycle      1004832
    Average SMSP Active Cycles       cycle      7237.41
    Total SMSP Elapsed Cycles        cycle      4019328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 13.28%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 23.06% above the average, while the minimum instance value is 100.00% below the average.    

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.21
    Branch Instructions              inst        38400
    Branch Efficiency                   %        64.41
    Avg. Divergent Branches                      16.80
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 8.16%                                                                                           
          This kernel has uncoalesced shared accesses resulting in a total of 4608 excessive wavefronts (10% of the     
          total 44544 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.     
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.09
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle         4120
    Memory Throughput                 %         0.53
    DRAM Throughput                   %         0.27
    Duration                         us         1.82
    L1/TEX Cache Throughput           %        36.17
    L2 Cache Throughput               %         0.53
    SM Active Cycles              cycle        33.17
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.08
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.37
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.37
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.72%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         1.89
    Mem Busy                               %         0.53
    Max Bandwidth                          %         0.39
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.13
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.74
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.26
    Active Warps Per Scheduler          warp         1.01
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.5 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.83
    Warp Cycles Per Executed Instruction           cycle        45.49
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 76.03%                                                                                          
          On average, each warp of this workload spends 28.0 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 76.0% of the total average of 36.8 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         7.40
    Achieved Active Warps Per SM           warp         3.55
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 92.6%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (7.4%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           54
    Total DRAM Elapsed Cycles        cycle       161792
    Average L1 Active Cycles         cycle        33.17
    Total L1 Elapsed Cycles          cycle       329420
    Average L2 Active Cycles         cycle       150.03
    Total L2 Elapsed Cycles          cycle       112128
    Average SM Active Cycles         cycle        33.17
    Total SM Elapsed Cycles          cycle       329420
    Average SMSP Active Cycles       cycle        28.76
    Total SMSP Elapsed Cycles        cycle      1317680
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.18
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        15974
    Memory Throughput                 %         0.47
    DRAM Throughput                   %         0.37
    Duration                         us         7.01
    L1/TEX Cache Throughput           %        10.85
    L2 Cache Throughput               %         0.47
    SM Active Cycles              cycle       169.96
    Compute (SM) Throughput           %         0.04
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.08
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.09
    Issued Ipc Active     inst/cycle         0.08
    SM Busy                        %         2.09
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.3%                                                                                     
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.65
    Mem Busy                               %         0.47
    Max Bandwidth                          %         0.39
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.14
    Mem Pipes Busy                         %         0.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1227%                                                                                         
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1192%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.64%                                                                                           
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.652%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.00
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.00
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91%                                                                                       
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.15
    Warp Cycles Per Executed Instruction           cycle        11.27
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.42%                                                                                          
          On average, each warp of this workload spends 4.6 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.4% of the total average of 11.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01318%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         1.94
    Achieved Active Warps Per SM           warp         0.93
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91%                                                                                             
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (1.9%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          290
    Total DRAM Elapsed Cycles        cycle       626688
    Average L1 Active Cycles         cycle       169.96
    Total L1 Elapsed Cycles          cycle      1277456
    Average L2 Active Cycles         cycle       721.09
    Total L2 Elapsed Cycles          cycle       434208
    Average SM Active Cycles         cycle       169.96
    Total SM Elapsed Cycles          cycle      1277456
    Average SMSP Active Cycles       cycle        39.54
    Total SMSP Elapsed Cycles        cycle      5109824
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.604%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.564%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       162473
    Memory Throughput                 %        70.04
    DRAM Throughput                   %        70.04
    Duration                         us        70.88
    L1/TEX Cache Throughput           %        36.36
    L2 Cache Throughput               %        38.83
    SM Active Cycles              cycle    117220.93
    Compute (SM) Throughput           %        20.19
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.53
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        13.34
    Issued Ipc Active     inst/cycle         0.53
    SM Busy                        %        27.95
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.0%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       503.71
    Mem Busy                               %        38.83
    Max Bandwidth                          %        70.04
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.49
    Mem Pipes Busy                         %        13.94
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 31.74%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.56
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.44
    Active Warps Per Scheduler          warp         1.76
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.96%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.76 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.98
    Warp Cycles Per Executed Instruction           cycle        13.00
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.96%                                                                                          
          On average, each warp of this workload spends 4.5 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 34.5% of the total average of 13.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.39
    Issued Instructions                             inst      5003006
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.55
    Achieved Active Warps Per SM           warp         6.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.96%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       557862
    Total DRAM Elapsed Cycles        cycle      6372352
    Average L1 Active Cycles         cycle    117220.93
    Total L1 Elapsed Cycles          cycle     12986156
    Average L2 Active Cycles         cycle       120281
    Total L2 Elapsed Cycles          cycle      4418592
    Average SM Active Cycles         cycle    117220.93
    Total SM Elapsed Cycles          cycle     12986156
    Average SMSP Active Cycles       cycle    115270.43
    Total SMSP Elapsed Cycles        cycle     51944624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.69%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.81% above the average, while the minimum instance value is 17.07% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.57%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.30% above the average, while the minimum instance value is 19.17% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.69%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.81% above the average, while the minimum instance value is 17.07% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1068%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038574 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        70303
    Memory Throughput                 %        66.69
    DRAM Throughput                   %        66.69
    Duration                         us        30.66
    L1/TEX Cache Throughput           %        14.57
    L2 Cache Throughput               %        24.01
    SM Active Cycles              cycle     61838.31
    Compute (SM) Throughput           %        41.82
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.90
    Executed Ipc Elapsed  inst/cycle         1.67
    Issue Slots Busy               %        47.53
    Issued Ipc Active     inst/cycle         1.90
    SM Busy                        %        47.53
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       479.05
    Mem Busy                               %        18.08
    Max Bandwidth                          %        66.69
    L1/TEX Hit Rate                        %        15.34
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.82
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.34
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        50.66
    Active Warps Per Scheduler          warp         7.73
    Eligible Warps Per Scheduler        warp         1.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 33.31%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.73 active warps per scheduler, but only an average of 1.06 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.66
    Warp Cycles Per Executed Instruction           cycle        15.68
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 33.31%                                                                                          
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 52.7% of the total average of 15.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.32
    Issued Instructions                             inst      9404903
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        61.96
    Achieved Active Warps Per SM           warp        29.74
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 33.31%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229464
    Total DRAM Elapsed Cycles        cycle      2752512
    Average L1 Active Cycles         cycle     61838.31
    Total L1 Elapsed Cycles          cycle      5622276
    Average L2 Active Cycles         cycle     55679.75
    Total L2 Elapsed Cycles          cycle      1911456
    Average SM Active Cycles         cycle     61838.31
    Total SM Elapsed Cycles          cycle      5622276
    Average SMSP Active Cycles       cycle     59569.45
    Total SMSP Elapsed Cycles        cycle     22489104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.057%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.02% above the average, while the minimum instance value is 6.38% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.699%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.08% above the average, while the minimum instance value is 7.67% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.057%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.02% above the average, while the minimum instance value is 6.38% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       249873
    Memory Throughput                 %        84.75
    DRAM Throughput                   %        84.75
    Duration                         us       108.93
    L1/TEX Cache Throughput           %        33.18
    L2 Cache Throughput               %        44.08
    SM Active Cycles              cycle    224839.65
    Compute (SM) Throughput           %        22.95
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.44
    Executed Ipc Elapsed  inst/cycle         0.40
    Issue Slots Busy               %        11.09
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.50
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.5%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       609.34
    Mem Busy                               %        44.08
    Max Bandwidth                          %        84.75
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.52
    Mem Pipes Busy                         %        15.19
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.38%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.38
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.62
    Active Warps Per Scheduler          warp         1.95
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 15.25%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.95 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.15
    Warp Cycles Per Executed Instruction           cycle        17.17
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 15.25%                                                                                          
          On average, each warp of this workload spends 7.1 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.2% of the total average of 17.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.62
    Issued Instructions                             inst      7979400
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9197%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.76
    Achieved Active Warps Per SM           warp         7.56
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 15.25%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1037104
    Total DRAM Elapsed Cycles        cycle      9789440
    Average L1 Active Cycles         cycle    224839.65
    Total L1 Elapsed Cycles          cycle     19985364
    Average L2 Active Cycles         cycle    195742.25
    Total L2 Elapsed Cycles          cycle      6793600
    Average SM Active Cycles         cycle    224839.65
    Total SM Elapsed Cycles          cycle     19985364
    Average SMSP Active Cycles       cycle    219211.73
    Total SMSP Elapsed Cycles        cycle     79941456
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.569%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.19% above the average, while the minimum instance value is 17.23% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.401%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 7.29% above the average, while the minimum instance value is 17.49% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.569%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.19% above the average, while the minimum instance value is 17.23% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03228%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5987448 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.077%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5254996 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        27854
    Memory Throughput                 %        48.83
    DRAM Throughput                   %        48.83
    Duration                         us        12.16
    L1/TEX Cache Throughput           %        23.67
    L2 Cache Throughput               %        35.27
    SM Active Cycles              cycle     24283.38
    Compute (SM) Throughput           %        26.43
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.18
    Executed Ipc Elapsed  inst/cycle         1.03
    Issue Slots Busy               %        30.27
    Issued Ipc Active     inst/cycle         1.21
    SM Busy                        %        30.27
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (24.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       350.68
    Mem Busy                               %        22.73
    Max Bandwidth                          %        48.83
    L1/TEX Hit Rate                        %        38.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.62
    Mem Pipes Busy                         %         5.89
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.75%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        29.64
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        70.36
    Active Warps Per Scheduler          warp         8.88
    Eligible Warps Per Scheduler        warp         0.70
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.17%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.88 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.95
    Warp Cycles Per Executed Instruction           cycle        30.83
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 43.7%                                                                                           
          On average, each warp of this workload spends 13.1 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 43.7% of the total average of 30.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7350.42
    Issued Instructions                             inst      2352135
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.904%                                                                                          
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        76.66
    Achieved Active Warps Per SM           warp        36.80
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66630
    Total DRAM Elapsed Cycles        cycle      1091584
    Average L1 Active Cycles         cycle     24283.38
    Total L1 Elapsed Cycles          cycle      2224656
    Average L2 Active Cycles         cycle     17913.12
    Total L2 Elapsed Cycles          cycle       757440
    Average SM Active Cycles         cycle     24283.38
    Total SM Elapsed Cycles          cycle      2224656
    Average SMSP Active Cycles       cycle     24797.58
    Total SMSP Elapsed Cycles        cycle      8898624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.84%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (131072, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       759629
    Memory Throughput                 %        92.85
    DRAM Throughput                   %        92.85
    Duration                         us       331.04
    L1/TEX Cache Throughput           %        27.61
    L2 Cache Throughput               %        40.63
    SM Active Cycles              cycle    753310.44
    Compute (SM) Throughput           %         3.67
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.15
    Executed Ipc Elapsed  inst/cycle         0.15
    Issue Slots Busy               %         3.70
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %         3.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.83%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       667.86
    Mem Busy                               %        39.14
    Max Bandwidth                          %        92.85
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %       100.00
    Mem Pipes Busy                         %         3.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.71
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.29
    Active Warps Per Scheduler          warp        10.66
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.155%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.66 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       287.54
    Warp Cycles Per Executed Instruction           cycle       287.96
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.155%                                                                                          
          On average, each warp of this workload spends 117.9 cycles being stalled after EXIT waiting for all           
          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  
          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  
          sure the memory access patterns of these store operations are optimal for the target architecture and         
          consider parallelized data reduction, if applicable. This stall type represents about 41.0% of the total      
          average of 287.5 cycles between issuing two instructions.                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     27852.80
    Executed Instructions                           inst      8912896
    Avg. Issued Instructions Per Scheduler          inst     27893.13
    Issued Instructions                             inst      8925803
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        16777216
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              136.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.77
    Achieved Active Warps Per SM           warp        42.61
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.155%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3454492
    Total DRAM Elapsed Cycles        cycle     29765632
    Average L1 Active Cycles         cycle    753310.44
    Total L1 Elapsed Cycles          cycle     60769508
    Average L2 Active Cycles         cycle    643328.03
    Total L2 Elapsed Cycles          cycle     20653792
    Average SM Active Cycles         cycle    753310.44
    Total SM Elapsed Cycles          cycle     60769508
    Average SMSP Active Cycles       cycle    752624.32
    Total SMSP Elapsed Cycles        cycle    243078032
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst      1048576
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.02
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle         3549
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.30
    Duration                         us         1.57
    L1/TEX Cache Throughput           %        40.57
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        29.57
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.66
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %         2.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.56%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.12
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.45
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.82
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.75
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.25
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.25%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.46
    Warp Cycles Per Executed Instruction           cycle        45.03
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 75.11%                                                                                          
          On average, each warp of this workload spends 27.4 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 75.1% of the total average of 36.5 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         8.25
    Achieved Active Warps Per SM           warp         3.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.75%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (8.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           52
    Total DRAM Elapsed Cycles        cycle       138240
    Average L1 Active Cycles         cycle        29.57
    Total L1 Elapsed Cycles          cycle       283652
    Average L2 Active Cycles         cycle       146.22
    Total L2 Elapsed Cycles          cycle        96480
    Average SM Active Cycles         cycle        29.57
    Total SM Elapsed Cycles          cycle       283652
    Average SMSP Active Cycles       cycle        28.61
    Total SMSP Elapsed Cycles        cycle      1134608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.17
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        15046
    Memory Throughput                 %         0.49
    DRAM Throughput                   %         0.39
    Duration                         us         6.59
    L1/TEX Cache Throughput           %        11.62
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle       158.65
    Compute (SM) Throughput           %         0.04
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.24
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.24
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.18%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.78
    Mem Busy                               %         0.49
    Max Bandwidth                          %         0.42
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        86.98
    Mem Pipes Busy                         %         0.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1303%                                                                                         
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1266%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.971%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.126%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.94
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.06
    Active Warps Per Scheduler          warp         1.02
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.06%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.2 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.02 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.38
    Warp Cycles Per Executed Instruction           cycle        11.50
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 40.95%                                                                                          
          On average, each warp of this workload spends 4.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 40.9% of the total average of 11.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01399%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.08
    Achieved Active Warps Per SM           warp         1.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.06%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       588800
    Average L1 Active Cycles         cycle       158.65
    Total L1 Elapsed Cycles          cycle      1203436
    Average L2 Active Cycles         cycle       714.47
    Total L2 Elapsed Cycles          cycle       408928
    Average SM Active Cycles         cycle       158.65
    Total SM Elapsed Cycles          cycle      1203436
    Average SMSP Active Cycles       cycle        39.80
    Total SMSP Elapsed Cycles        cycle      4813744
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.844%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.5588%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       161499
    Memory Throughput                 %        70.87
    DRAM Throughput                   %        70.87
    Duration                         us        70.50
    L1/TEX Cache Throughput           %        36.89
    L2 Cache Throughput               %        39.06
    SM Active Cycles              cycle    115506.94
    Compute (SM) Throughput           %        20.31
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.39
    Issue Slots Busy               %        13.54
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.37
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       509.21
    Mem Busy                               %        39.06
    Max Bandwidth                          %        70.87
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        14.03
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 31.93%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.56
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.44
    Active Warps Per Scheduler          warp         1.75
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.75 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.94
    Warp Cycles Per Executed Instruction           cycle        12.96
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.13%                                                                                          
          On average, each warp of this workload spends 4.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 33.2% of the total average of 12.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.40
    Issued Instructions                             inst      5003008
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.69
    Achieved Active Warps Per SM           warp         7.05
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.13%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       560896
    Total DRAM Elapsed Cycles        cycle      6331392
    Average L1 Active Cycles         cycle    115506.94
    Total L1 Elapsed Cycles          cycle     12908212
    Average L2 Active Cycles         cycle    114382.69
    Total L2 Elapsed Cycles          cycle      4391936
    Average SM Active Cycles         cycle    115506.94
    Total SM Elapsed Cycles          cycle     12908212
    Average SMSP Active Cycles       cycle    115339.88
    Total SMSP Elapsed Cycles        cycle     51632848
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.53%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 17.50% above the average, while the minimum instance value is 17.49% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.87%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 16.61% above the average, while the minimum instance value is 18.61% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.53%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 17.50% above the average, while the minimum instance value is 17.49% below  
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1059%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038570 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69905
    Memory Throughput                 %        67.04
    DRAM Throughput                   %        67.04
    Duration                         us        30.50
    L1/TEX Cache Throughput           %        14.81
    L2 Cache Throughput               %        24.15
    SM Active Cycles              cycle     60850.95
    Compute (SM) Throughput           %        42.06
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.93
    Executed Ipc Elapsed  inst/cycle         1.68
    Issue Slots Busy               %        48.30
    Issued Ipc Active     inst/cycle         1.93
    SM Busy                        %        48.30
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       481.55
    Mem Busy                               %        18.18
    Max Bandwidth                          %        67.04
    L1/TEX Hit Rate                        %        15.33
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.90
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.33
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        50.67
    Active Warps Per Scheduler          warp         7.71
    Eligible Warps Per Scheduler        warp         1.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.96%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.71 active warps per scheduler, but only an average of 1.05 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.64
    Warp Cycles Per Executed Instruction           cycle        15.66
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.96%                                                                                          
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.1% of the total average of 15.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.37
    Issued Instructions                             inst      9404917
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.48
    Achieved Active Warps Per SM           warp        29.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.96%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229458
    Total DRAM Elapsed Cycles        cycle      2738176
    Average L1 Active Cycles         cycle     60850.95
    Total L1 Elapsed Cycles          cycle      5590064
    Average L2 Active Cycles         cycle     55937.94
    Total L2 Elapsed Cycles          cycle      1900448
    Average SM Active Cycles         cycle     60850.95
    Total SM Elapsed Cycles          cycle      5590064
    Average SMSP Active Cycles       cycle     59582.72
    Total SMSP Elapsed Cycles        cycle     22360256
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.557%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.68% above the average, while the minimum instance value is 6.81% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.797%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.14% above the average, while the minimum instance value is 7.86% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.557%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.68% above the average, while the minimum instance value is 6.81% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       259329
    Memory Throughput                 %        82.99
    DRAM Throughput                   %        82.99
    Duration                         us       113.06
    L1/TEX Cache Throughput           %        32.82
    L2 Cache Throughput               %        42.48
    SM Active Cycles              cycle    227253.81
    Compute (SM) Throughput           %        22.12
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.44
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        10.97
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.23
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.2%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       596.77
    Mem Busy                               %        42.48
    Max Bandwidth                          %        82.99
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.52
    Mem Pipes Busy                         %        14.63
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 31.19%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.42
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.58
    Active Warps Per Scheduler          warp         1.95
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 17.01%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.95 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.11
    Warp Cycles Per Executed Instruction           cycle        17.12
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.01%                                                                                          
          On average, each warp of this workload spends 7.0 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.1% of the total average of 17.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.68
    Issued Instructions                             inst      7979417
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9099%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.83
    Achieved Active Warps Per SM           warp         7.60
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 17.01%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1054192
    Total DRAM Elapsed Cycles        cycle     10162176
    Average L1 Active Cycles         cycle    227253.81
    Total L1 Elapsed Cycles          cycle     20741424
    Average L2 Active Cycles         cycle    196013.47
    Total L2 Elapsed Cycles          cycle      7050848
    Average SM Active Cycles         cycle    227253.81
    Total SM Elapsed Cycles          cycle     20741424
    Average SMSP Active Cycles       cycle    218399.79
    Total SMSP Elapsed Cycles        cycle     82965696
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.586%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 8.65% above the average, while the minimum instance value is 17.23% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.183%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.53% above the average, while the minimum instance value is 15.22% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.586%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 8.65% above the average, while the minimum instance value is 17.23% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03114%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5988728 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.07499%                                                                                        
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5255014 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.19
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        28119
    Memory Throughput                 %        48.33
    DRAM Throughput                   %        48.33
    Duration                         us        12.32
    L1/TEX Cache Throughput           %        23.27
    L2 Cache Throughput               %        34.62
    SM Active Cycles              cycle     24591.56
    Compute (SM) Throughput           %        26.18
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.16
    Executed Ipc Elapsed  inst/cycle         1.02
    Issue Slots Busy               %        29.90
    Issued Ipc Active     inst/cycle         1.20
    SM Busy                        %        29.90
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (23.7%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       346.13
    Mem Busy                               %        22.62
    Max Bandwidth                          %        48.33
    L1/TEX Hit Rate                        %        38.87
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        60.99
    Mem Pipes Busy                         %         5.84
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.45%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        30.02
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        69.98
    Active Warps Per Scheduler          warp         8.82
    Eligible Warps Per Scheduler        warp         0.69
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.67%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.82 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.38
    Warp Cycles Per Executed Instruction           cycle        30.24
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.21%                                                                                          
          On average, each warp of this workload spends 13.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 44.2% of the total average of 29.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7352.00
    Issued Instructions                             inst      2352641
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.83%                                                                                           
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        72.92
    Achieved Active Warps Per SM           warp        35.00
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.5%                                                                                           
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (72.9%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66630
    Total DRAM Elapsed Cycles        cycle      1102848
    Average L1 Active Cycles         cycle     24591.56
    Total L1 Elapsed Cycles          cycle      2246296
    Average L2 Active Cycles         cycle     17981.44
    Total L2 Elapsed Cycles          cycle       764320
    Average SM Active Cycles         cycle     24591.56
    Total SM Elapsed Cycles          cycle      2246296
    Average SMSP Active Cycles       cycle     24488.56
    Total SMSP Elapsed Cycles        cycle      8985184
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.64%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (131072, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       760812
    Memory Throughput                 %        92.86
    DRAM Throughput                   %        92.86
    Duration                         us       331.55
    L1/TEX Cache Throughput           %        27.57
    L2 Cache Throughput               %        41.97
    SM Active Cycles              cycle    753095.80
    Compute (SM) Throughput           %         3.67
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.15
    Executed Ipc Elapsed  inst/cycle         0.15
    Issue Slots Busy               %         3.70
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %         3.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.82%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       667.91
    Mem Busy                               %        41.97
    Max Bandwidth                          %        92.86
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %       100.00
    Mem Pipes Busy                         %         3.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.71
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.29
    Active Warps Per Scheduler          warp        10.63
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.141%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.63 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       286.72
    Warp Cycles Per Executed Instruction           cycle       287.12
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.141%                                                                                          
          On average, each warp of this workload spends 117.6 cycles being stalled after EXIT waiting for all           
          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  
          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  
          sure the memory access patterns of these store operations are optimal for the target architecture and         
          consider parallelized data reduction, if applicable. This stall type represents about 41.0% of the total      
          average of 286.7 cycles between issuing two instructions.                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     27852.80
    Executed Instructions                           inst      8912896
    Avg. Issued Instructions Per Scheduler          inst     27891.64
    Issued Instructions                             inst      8925324
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        16777216
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              136.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.47
    Achieved Active Warps Per SM           warp        42.47
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.141%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3460102
    Total DRAM Elapsed Cycles        cycle     29809664
    Average L1 Active Cycles         cycle    753095.80
    Total L1 Elapsed Cycles          cycle     60864276
    Average L2 Active Cycles         cycle    643121.50
    Total L2 Elapsed Cycles          cycle     20685856
    Average SM Active Cycles         cycle    753095.80
    Total SM Elapsed Cycles          cycle     60864276
    Average SMSP Active Cycles       cycle    752422.13
    Total SMSP Elapsed Cycles        cycle    243457104
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst      1048576
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        10.88
    SM Frequency                    Ghz         2.20
    Elapsed Cycles                cycle         3520
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.29
    Duration                         us         1.60
    L1/TEX Cache Throughput           %        40.35
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        29.74
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %         2.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.57%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s            2
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.46
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.81
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.74
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.26
    Active Warps Per Scheduler          warp         1.01
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.26%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.94
    Warp Cycles Per Executed Instruction           cycle        45.63
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 75.5%                                                                                           
          On average, each warp of this workload spends 27.9 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 75.5% of the total average of 36.9 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         9.12
    Achieved Active Warps Per SM           warp         4.38
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 90.88%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (9.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           50
    Total DRAM Elapsed Cycles        cycle       139264
    Average L1 Active Cycles         cycle        29.74
    Total L1 Elapsed Cycles          cycle       281228
    Average L2 Active Cycles         cycle       149.88
    Total L2 Elapsed Cycles          cycle        95552
    Average SM Active Cycles         cycle        29.74
    Total SM Elapsed Cycles          cycle       281228
    Average SMSP Active Cycles       cycle        28.79
    Total SMSP Elapsed Cycles        cycle      1124912
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.15
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        15135
    Memory Throughput                 %         0.49
    DRAM Throughput                   %         0.39
    Duration                         us         6.62
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle       159.54
    Compute (SM) Throughput           %         0.04
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.23
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.23
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.19%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.76
    Mem Busy                               %         0.49
    Max Bandwidth                          %         0.41
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.44
    Mem Pipes Busy                         %         0.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1295%                                                                                         
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1258%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.943%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.087%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.96
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.04
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.04%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.2 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.19
    Warp Cycles Per Executed Instruction           cycle        11.31
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.24%                                                                                          
          On average, each warp of this workload spends 4.6 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.2% of the total average of 11.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01391%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.04%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       590848
    Average L1 Active Cycles         cycle       159.54
    Total L1 Elapsed Cycles          cycle      1210464
    Average L2 Active Cycles         cycle       738.31
    Total L2 Elapsed Cycles          cycle       411264
    Average SM Active Cycles         cycle       159.54
    Total SM Elapsed Cycles          cycle      1210464
    Average SMSP Active Cycles       cycle        39.73
    Total SMSP Elapsed Cycles        cycle      4841856
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.977%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.5587%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       150380
    Memory Throughput                 %        76.24
    DRAM Throughput                   %        76.24
    Duration                         us        65.60
    L1/TEX Cache Throughput           %        36.78
    L2 Cache Throughput               %        41.95
    SM Active Cycles              cycle    115856.09
    Compute (SM) Throughput           %        21.81
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        13.49
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.28
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.3%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       548.13
    Mem Busy                               %        41.95
    Max Bandwidth                          %        76.24
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        15.07
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 34.3%                                                                                           
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.73
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.27
    Active Warps Per Scheduler          warp         1.78
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 23.76%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.78 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.97
    Warp Cycles Per Executed Instruction           cycle        12.99
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.76%                                                                                          
          On average, each warp of this workload spends 4.5 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 34.6% of the total average of 13.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.40
    Issued Instructions                             inst      5003007
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.63
    Achieved Active Warps Per SM           warp         7.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 23.76%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       561832
    Total DRAM Elapsed Cycles        cycle      5895168
    Average L1 Active Cycles         cycle    115856.09
    Total L1 Elapsed Cycles          cycle     12016832
    Average L2 Active Cycles         cycle    118893.78
    Total L2 Elapsed Cycles          cycle      4088672
    Average SM Active Cycles         cycle    115856.09
    Total SM Elapsed Cycles          cycle     12016832
    Average SMSP Active Cycles       cycle    113875.10
    Total SMSP Elapsed Cycles        cycle     48067328
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.68%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 13.85% above the average, while the minimum instance value is 17.50% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.36%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 16.30% above the average, while the minimum instance value is 16.88% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.68%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 13.85% above the average, while the minimum instance value is 17.50% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1141%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038574 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69695
    Memory Throughput                 %        67.22
    DRAM Throughput                   %        67.22
    Duration                         us        30.40
    L1/TEX Cache Throughput           %        14.77
    L2 Cache Throughput               %        24.22
    SM Active Cycles              cycle     61027.07
    Compute (SM) Throughput           %        42.19
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.92
    Executed Ipc Elapsed  inst/cycle         1.69
    Issue Slots Busy               %        48.16
    Issued Ipc Active     inst/cycle         1.93
    SM Busy                        %        48.16
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       483.07
    Mem Busy                               %        18.24
    Max Bandwidth                          %        67.22
    L1/TEX Hit Rate                        %        15.23
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.93
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.18
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        50.82
    Active Warps Per Scheduler          warp         7.65
    Eligible Warps Per Scheduler        warp         1.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.78%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.65 active warps per scheduler, but only an average of 1.05 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.56
    Warp Cycles Per Executed Instruction           cycle        15.58
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.78%                                                                                          
          On average, each warp of this workload spends 8.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.8% of the total average of 15.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.36
    Issued Instructions                             inst      9404915
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.88
    Achieved Active Warps Per SM           warp        30.18
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.78%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229458
    Total DRAM Elapsed Cycles        cycle      2731008
    Average L1 Active Cycles         cycle     61027.07
    Total L1 Elapsed Cycles          cycle      5573276
    Average L2 Active Cycles         cycle     56184.84
    Total L2 Elapsed Cycles          cycle      1894528
    Average SM Active Cycles         cycle     61027.07
    Total SM Elapsed Cycles          cycle      5573276
    Average SMSP Active Cycles       cycle     59758.25
    Total SMSP Elapsed Cycles        cycle     22293104
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.793%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 7.75% above the average, while the minimum instance value is 6.32% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.517%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.76% above the average, while the minimum instance value is 7.77% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.793%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.75% above the average, while the minimum instance value is 6.32% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       249653
    Memory Throughput                 %        86.22
    DRAM Throughput                   %        86.22
    Duration                         us       108.83
    L1/TEX Cache Throughput           %        32.55
    L2 Cache Throughput               %        44.12
    SM Active Cycles              cycle    229187.15
    Compute (SM) Throughput           %        22.98
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.43
    Executed Ipc Elapsed  inst/cycle         0.40
    Issue Slots Busy               %        10.88
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.02
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       619.92
    Mem Busy                               %        44.12
    Max Bandwidth                          %        86.22
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.52
    Mem Pipes Busy                         %        15.20
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.4%                                                                                           
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.41
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.59
    Active Warps Per Scheduler          warp         1.97
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.78%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.97 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.26
    Warp Cycles Per Executed Instruction           cycle        17.27
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.78%                                                                                          
          On average, each warp of this workload spends 6.8 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 39.2% of the total average of 17.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.57
    Issued Instructions                             inst      7979381
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9022%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.75
    Achieved Active Warps Per SM           warp         7.56
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.78%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1054172
    Total DRAM Elapsed Cycles        cycle      9781248
    Average L1 Active Cycles         cycle    229187.15
    Total L1 Elapsed Cycles          cycle     19967108
    Average L2 Active Cycles         cycle    197566.16
    Total L2 Elapsed Cycles          cycle      6787840
    Average SM Active Cycles         cycle    229187.15
    Total SM Elapsed Cycles          cycle     19967108
    Average SMSP Active Cycles       cycle    218555.56
    Total SMSP Elapsed Cycles        cycle     79868432
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.417%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 5.90% above the average, while the minimum instance value is 13.76% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.71%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.52% above the average, while the minimum instance value is 13.88% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.417%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 5.90% above the average, while the minimum instance value is 13.76% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0326%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5988576 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.07856%                                                                                        
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5255000 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.17
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        27855
    Memory Throughput                 %        48.79
    DRAM Throughput                   %        48.79
    Duration                         us        12.22
    L1/TEX Cache Throughput           %        23.69
    L2 Cache Throughput               %        35.15
    SM Active Cycles              cycle     24514.76
    Compute (SM) Throughput           %        26.42
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.17
    Executed Ipc Elapsed  inst/cycle         1.03
    Issue Slots Busy               %        29.99
    Issued Ipc Active     inst/cycle         1.20
    SM Busy                        %        29.99
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (23.8%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       348.86
    Mem Busy                               %        22.71
    Max Bandwidth                          %        48.79
    L1/TEX Hit Rate                        %        39.29
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.43
    Mem Pipes Busy                         %         5.89
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.77%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        30.19
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        69.81
    Active Warps Per Scheduler          warp         8.92
    Eligible Warps Per Scheduler        warp         0.69
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.21%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.92 active warps per scheduler, but only an average of 0.69 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.55
    Warp Cycles Per Executed Instruction           cycle        30.41
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 42.5%                                                                                           
          On average, each warp of this workload spends 12.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 42.5% of the total average of 29.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7351.27
    Issued Instructions                             inst      2352405
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.848%                                                                                          
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        76.26
    Achieved Active Warps Per SM           warp        36.60
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66632
    Total DRAM Elapsed Cycles        cycle      1092608
    Average L1 Active Cycles         cycle     24514.76
    Total L1 Elapsed Cycles          cycle      2225592
    Average L2 Active Cycles         cycle     17915.66
    Total L2 Elapsed Cycles          cycle       757728
    Average SM Active Cycles         cycle     24514.76
    Total SM Elapsed Cycles          cycle      2225592
    Average SMSP Active Cycles       cycle     24351.40
    Total SMSP Elapsed Cycles        cycle      8902368
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.651%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.46% above the average, while the minimum instance value is 10.12% below the average.      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.83%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (131072, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       761991
    Memory Throughput                 %        92.88
    DRAM Throughput                   %        92.88
    Duration                         us       332.06
    L1/TEX Cache Throughput           %        27.52
    L2 Cache Throughput               %        41.42
    SM Active Cycles              cycle    753501.45
    Compute (SM) Throughput           %         3.66
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.15
    Executed Ipc Elapsed  inst/cycle         0.15
    Issue Slots Busy               %         3.70
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %         3.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.83%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       668.06
    Mem Busy                               %        41.42
    Max Bandwidth                          %        92.88
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        99.99
    Mem Pipes Busy                         %         3.44
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.72
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.28
    Active Warps Per Scheduler          warp        10.63
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.124%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 26.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.63 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       285.93
    Warp Cycles Per Executed Instruction           cycle       286.33
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.124%                                                                                          
          On average, each warp of this workload spends 117.6 cycles being stalled after EXIT waiting for all           
          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  
          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  
          sure the memory access patterns of these store operations are optimal for the target architecture and         
          consider parallelized data reduction, if applicable. This stall type represents about 41.1% of the total      
          average of 285.9 cycles between issuing two instructions.                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     27852.80
    Executed Instructions                           inst      8912896
    Avg. Issued Instructions Per Scheduler          inst     27891.88
    Issued Instructions                             inst      8925400
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        16777216
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              136.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.72
    Achieved Active Warps Per SM           warp        42.59
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.124%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.7%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3466222
    Total DRAM Elapsed Cycles        cycle     29856768
    Average L1 Active Cycles         cycle    753501.45
    Total L1 Elapsed Cycles          cycle     60958656
    Average L2 Active Cycles         cycle    641500.16
    Total L2 Elapsed Cycles          cycle     20717696
    Average SM Active Cycles         cycle    753501.45
    Total SM Elapsed Cycles          cycle     60958656
    Average SMSP Active Cycles       cycle    750242.41
    Total SMSP Elapsed Cycles        cycle    243834624
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst      1048576
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        10.86
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle         3525
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.29
    Duration                         us         1.57
    L1/TEX Cache Throughput           %        40.35
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        29.74
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %         2.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.57%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.04
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.46
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.82
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.76
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.24
    Active Warps Per Scheduler          warp         1.02
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.24%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.02 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.96
    Warp Cycles Per Executed Instruction           cycle        45.66
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 74.89%                                                                                          
          On average, each warp of this workload spends 27.7 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 74.9% of the total average of 37.0 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         8.29
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.71%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (8.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           50
    Total DRAM Elapsed Cycles        cycle       136192
    Average L1 Active Cycles         cycle        29.74
    Total L1 Elapsed Cycles          cycle       281708
    Average L2 Active Cycles         cycle       148.88
    Total L2 Elapsed Cycles          cycle        95808
    Average SM Active Cycles         cycle        29.74
    Total SM Elapsed Cycles          cycle       281708
    Average SMSP Active Cycles       cycle        28.56
    Total SMSP Elapsed Cycles        cycle      1126832
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        15306
    Memory Throughput                 %         0.49
    DRAM Throughput                   %         0.38
    Duration                         us         6.69
    L1/TEX Cache Throughput           %        11.41
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle       161.55
    Compute (SM) Throughput           %         0.04
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.20
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.20
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.21%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.74
    Mem Busy                               %         0.49
    Max Bandwidth                          %         0.41
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.40
    Mem Pipes Busy                         %         0.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1281%                                                                                         
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1244%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.882%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.998%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.03
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        90.97
    Active Warps Per Scheduler          warp         1.01
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.97%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.01 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.16
    Warp Cycles Per Executed Instruction           cycle        11.28
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 42.68%                                                                                          
          On average, each warp of this workload spends 4.8 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 42.7% of the total average of 11.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01375%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.05
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 90.97%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       600064
    Average L1 Active Cycles         cycle       161.55
    Total L1 Elapsed Cycles          cycle      1224284
    Average L2 Active Cycles         cycle       719.09
    Total L2 Elapsed Cycles          cycle       415904
    Average SM Active Cycles         cycle       161.55
    Total SM Elapsed Cycles          cycle      1224284
    Average SMSP Active Cycles       cycle        39.42
    Total SMSP Elapsed Cycles        cycle      4897136
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.794%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.5594%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       162503
    Memory Throughput                 %        70.57
    DRAM Throughput                   %        70.57
    Duration                         us        70.88
    L1/TEX Cache Throughput           %        36.97
    L2 Cache Throughput               %        38.83
    SM Active Cycles              cycle    115283.41
    Compute (SM) Throughput           %        20.19
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.38
    Issue Slots Busy               %        13.56
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.42
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       507.53
    Mem Busy                               %        38.83
    Max Bandwidth                          %        70.57
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        13.94
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 31.74%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.59
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.41
    Active Warps Per Scheduler          warp         1.76
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 29.43%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.76 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.92
    Warp Cycles Per Executed Instruction           cycle        12.94
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.43%                                                                                          
          On average, each warp of this workload spends 4.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 34.3% of the total average of 12.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.40
    Issued Instructions                             inst      5003007
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.63
    Achieved Active Warps Per SM           warp         7.02
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 29.43%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       562088
    Total DRAM Elapsed Cycles        cycle      6372352
    Average L1 Active Cycles         cycle    115283.41
    Total L1 Elapsed Cycles          cycle     12986996
    Average L2 Active Cycles         cycle    117534.09
    Total L2 Elapsed Cycles          cycle      4418144
    Average SM Active Cycles         cycle    115283.41
    Total SM Elapsed Cycles          cycle     12986996
    Average SMSP Active Cycles       cycle    115062.96
    Total SMSP Elapsed Cycles        cycle     51947984
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.56%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 16.28% above the average, while the minimum instance value is 19.38% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 13.94%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Maximum instance value is 19.66% above the average, while the minimum instance value is 16.67% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.56%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 16.28% above the average, while the minimum instance value is 19.38% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1051%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038572 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.21
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69916
    Memory Throughput                 %        67.04
    DRAM Throughput                   %        67.04
    Duration                         us        30.53
    L1/TEX Cache Throughput           %        14.73
    L2 Cache Throughput               %        24.14
    SM Active Cycles              cycle     61157.46
    Compute (SM) Throughput           %        42.05
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.92
    Executed Ipc Elapsed  inst/cycle         1.68
    Issue Slots Busy               %        48.06
    Issued Ipc Active     inst/cycle         1.92
    SM Busy                        %        48.06
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       481.04
    Mem Busy                               %        18.18
    Max Bandwidth                          %        67.04
    L1/TEX Hit Rate                        %        15.27
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.89
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.00
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        51.00
    Active Warps Per Scheduler          warp         7.66
    Eligible Warps Per Scheduler        warp         1.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.96%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.66 active warps per scheduler, but only an average of 1.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.64
    Warp Cycles Per Executed Instruction           cycle        15.66
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.96%                                                                                          
          On average, each warp of this workload spends 8.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.5% of the total average of 15.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.38
    Issued Instructions                             inst      9404921
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.11
    Achieved Active Warps Per SM           warp        29.81
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.96%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229458
    Total DRAM Elapsed Cycles        cycle      2738176
    Average L1 Active Cycles         cycle     61157.46
    Total L1 Elapsed Cycles          cycle      5590988
    Average L2 Active Cycles         cycle     55569.78
    Total L2 Elapsed Cycles          cycle      1900800
    Average SM Active Cycles         cycle     61157.46
    Total SM Elapsed Cycles          cycle      5590988
    Average SMSP Active Cycles       cycle     59986.29
    Total SMSP Elapsed Cycles        cycle     22363952
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.302%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.34% above the average, while the minimum instance value is 6.51% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.814%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.10% above the average, while the minimum instance value is 7.72% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.302%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.34% above the average, while the minimum instance value is 6.51% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       250656
    Memory Throughput                 %        85.87
    DRAM Throughput                   %        85.87
    Duration                         us       109.28
    L1/TEX Cache Throughput           %        32.63
    L2 Cache Throughput               %        43.95
    SM Active Cycles              cycle    228622.41
    Compute (SM) Throughput           %        22.88
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.44
    Executed Ipc Elapsed  inst/cycle         0.40
    Issue Slots Busy               %        10.91
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.08
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       617.35
    Mem Busy                               %        43.95
    Max Bandwidth                          %        85.87
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.53
    Mem Pipes Busy                         %        15.14
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.27%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.40
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.60
    Active Warps Per Scheduler          warp         1.96
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.96 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.24
    Warp Cycles Per Executed Instruction           cycle        17.25
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.13%                                                                                          
          On average, each warp of this workload spends 6.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 38.8% of the total average of 17.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.56
    Issued Instructions                             inst      7979378
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9045%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.70
    Achieved Active Warps Per SM           warp         7.54
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.13%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1054126
    Total DRAM Elapsed Cycles        cycle      9820160
    Average L1 Active Cycles         cycle    228622.41
    Total L1 Elapsed Cycles          cycle     20047724
    Average L2 Active Cycles         cycle    195940.41
    Total L2 Elapsed Cycles          cycle      6814944
    Average SM Active Cycles         cycle    228622.41
    Total SM Elapsed Cycles          cycle     20047724
    Average SMSP Active Cycles       cycle    218761.52
    Total SMSP Elapsed Cycles        cycle     80190896
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.935%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.51% above the average, while the minimum instance value is 14.95% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.138%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.18% above the average, while the minimum instance value is 13.56% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.935%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.51% above the average, while the minimum instance value is 14.95% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03221%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5986872 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.07805%                                                                                        
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5255004 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.19
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        28064
    Memory Throughput                 %        48.47
    DRAM Throughput                   %        48.47
    Duration                         us        12.29
    L1/TEX Cache Throughput           %        23.15
    L2 Cache Throughput               %        35.11
    SM Active Cycles              cycle     24582.38
    Compute (SM) Throughput           %        26.23
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.16
    Executed Ipc Elapsed  inst/cycle         1.02
    Issue Slots Busy               %        29.90
    Issued Ipc Active     inst/cycle         1.20
    SM Busy                        %        29.90
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (23.7%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       347.03
    Mem Busy                               %        22.51
    Max Bandwidth                          %        48.47
    L1/TEX Hit Rate                        %        38.90
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.63
    Mem Pipes Busy                         %         5.85
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.36%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        30.39
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        69.61
    Active Warps Per Scheduler          warp         8.94
    Eligible Warps Per Scheduler        warp         0.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.94 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.41
    Warp Cycles Per Executed Instruction           cycle        30.27
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 43%                                                                                             
          On average, each warp of this workload spends 12.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 43.0% of the total average of 29.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7350.82
    Issued Instructions                             inst      2352262
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.832%                                                                                          
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        75.07
    Achieved Active Warps Per SM           warp        36.03
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66630
    Total DRAM Elapsed Cycles        cycle      1099776
    Average L1 Active Cycles         cycle     24582.38
    Total L1 Elapsed Cycles          cycle      2241960
    Average L2 Active Cycles         cycle     18199.19
    Total L2 Elapsed Cycles          cycle       763136
    Average SM Active Cycles         cycle     24582.38
    Total SM Elapsed Cycles          cycle      2241960
    Average SMSP Active Cycles       cycle     24189.22
    Total SMSP Elapsed Cycles        cycle      8967840
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 38.16%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (131072, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       760905
    Memory Throughput                 %        92.85
    DRAM Throughput                   %        92.85
    Duration                         us       331.58
    L1/TEX Cache Throughput           %        27.56
    L2 Cache Throughput               %        40.94
    SM Active Cycles              cycle    753133.90
    Compute (SM) Throughput           %         3.67
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.15
    Executed Ipc Elapsed  inst/cycle         0.15
    Issue Slots Busy               %         3.70
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %         3.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.82%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       667.93
    Mem Busy                               %        40.94
    Max Bandwidth                          %        92.85
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %       100.00
    Mem Pipes Busy                         %         3.45
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.71
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.29
    Active Warps Per Scheduler          warp        10.63
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.145%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 26.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.63 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       286.24
    Warp Cycles Per Executed Instruction           cycle       286.64
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.145%                                                                                          
          On average, each warp of this workload spends 117.7 cycles being stalled after EXIT waiting for all           
          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  
          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  
          sure the memory access patterns of these store operations are optimal for the target architecture and         
          consider parallelized data reduction, if applicable. This stall type represents about 41.1% of the total      
          average of 286.2 cycles between issuing two instructions.                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     27852.80
    Executed Instructions                           inst      8912896
    Avg. Issued Instructions Per Scheduler          inst     27891.83
    Issued Instructions                             inst      8925387
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        16777216
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              136.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.79
    Achieved Active Warps Per SM           warp        42.62
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.145%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3460550
    Total DRAM Elapsed Cycles        cycle     29814784
    Average L1 Active Cycles         cycle    753133.90
    Total L1 Elapsed Cycles          cycle     60871536
    Average L2 Active Cycles         cycle    642818.47
    Total L2 Elapsed Cycles          cycle     20688288
    Average SM Active Cycles         cycle    753133.90
    Total SM Elapsed Cycles          cycle     60871536
    Average SMSP Active Cycles       cycle    750936.43
    Total SMSP Elapsed Cycles        cycle    243486144
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst      1048576
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        10.94
    SM Frequency                    Ghz         2.25
    Elapsed Cycles                cycle         3534
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.29
    Duration                         us         1.57
    L1/TEX Cache Throughput           %        40.37
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        29.73
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.65
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %         2.65
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.57%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.04
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.45
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.38
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.71
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.29
    Active Warps Per Scheduler          warp         1.01
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.29%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.01 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        37.11
    Warp Cycles Per Executed Instruction           cycle        45.84
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 76.52%                                                                                          
          On average, each warp of this workload spends 28.4 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 76.5% of the total average of 37.1 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         8.30
    Achieved Active Warps Per SM           warp         3.98
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.7%                                                                                           
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (8.3%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           50
    Total DRAM Elapsed Cycles        cycle       137216
    Average L1 Active Cycles         cycle        29.73
    Total L1 Elapsed Cycles          cycle       282500
    Average L2 Active Cycles         cycle       151.12
    Total L2 Elapsed Cycles          cycle        95904
    Average SM Active Cycles         cycle        29.73
    Total SM Elapsed Cycles          cycle       282500
    Average SMSP Active Cycles       cycle        29.06
    Total SMSP Elapsed Cycles        cycle      1130000
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.18
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        15393
    Memory Throughput                 %         0.48
    DRAM Throughput                   %         0.38
    Duration                         us         6.75
    L1/TEX Cache Throughput           %        11.32
    L2 Cache Throughput               %         0.48
    SM Active Cycles              cycle       162.86
    Compute (SM) Throughput           %         0.04
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.19
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.19
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.22%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.71
    Mem Busy                               %         0.48
    Max Bandwidth                          %         0.41
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.37
    Mem Pipes Busy                         %         0.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1273%                                                                                         
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1237%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.842%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.942%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.00
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.00
    Active Warps Per Scheduler          warp         1.01
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91%                                                                                       
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.01 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.16
    Warp Cycles Per Executed Instruction           cycle        11.28
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 42.5%                                                                                           
          On average, each warp of this workload spends 4.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 42.5% of the total average of 11.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01367%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.07
    Achieved Active Warps Per SM           warp         0.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91%                                                                                             
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       604160
    Average L1 Active Cycles         cycle       162.86
    Total L1 Elapsed Cycles          cycle      1231304
    Average L2 Active Cycles         cycle       726.75
    Total L2 Elapsed Cycles          cycle       418656
    Average SM Active Cycles         cycle       162.86
    Total SM Elapsed Cycles          cycle      1231304
    Average SMSP Active Cycles       cycle        39.53
    Total SMSP Elapsed Cycles        cycle      4925216
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.813%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.5607%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       147920
    Memory Throughput                 %        77.48
    DRAM Throughput                   %        77.48
    Duration                         us        64.54
    L1/TEX Cache Throughput           %        36.80
    L2 Cache Throughput               %        42.65
    SM Active Cycles              cycle    115814.49
    Compute (SM) Throughput           %        22.17
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        13.50
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.29
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.3%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       557.01
    Mem Busy                               %        42.65
    Max Bandwidth                          %        77.48
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        15.31
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 34.86%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.60
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.40
    Active Warps Per Scheduler          warp         1.75
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.52%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.75 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.89
    Warp Cycles Per Executed Instruction           cycle        12.90
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 22.52%                                                                                          
          On average, each warp of this workload spends 4.6 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 35.9% of the total average of 12.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.40
    Issued Instructions                             inst      5003007
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.65
    Achieved Active Warps Per SM           warp         7.03
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 22.52%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       561746
    Total DRAM Elapsed Cycles        cycle      5799936
    Average L1 Active Cycles         cycle    115814.49
    Total L1 Elapsed Cycles          cycle     11821656
    Average L2 Active Cycles         cycle    120413.56
    Total L2 Elapsed Cycles          cycle      4022272
    Average SM Active Cycles         cycle    115814.49
    Total SM Elapsed Cycles          cycle     11821656
    Average SMSP Active Cycles       cycle    114939.01
    Total SMSP Elapsed Cycles        cycle     47286624
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 14.07%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 17.95% above the average, while the minimum instance value is 18.82% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.27%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 15.78% above the average, while the minimum instance value is 16.84% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 14.07%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 17.95% above the average, while the minimum instance value is 18.82% below  
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.116%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038574 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69432
    Memory Throughput                 %        67.49
    DRAM Throughput                   %        67.49
    Duration                         us        30.30
    L1/TEX Cache Throughput           %        14.82
    L2 Cache Throughput               %        24.31
    SM Active Cycles              cycle     60795.34
    Compute (SM) Throughput           %        42.35
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.93
    Executed Ipc Elapsed  inst/cycle         1.69
    Issue Slots Busy               %        48.34
    Issued Ipc Active     inst/cycle         1.93
    SM Busy                        %        48.34
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       484.60
    Mem Busy                               %        18.31
    Max Bandwidth                          %        67.49
    L1/TEX Hit Rate                        %        15.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.98
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.41
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        50.59
    Active Warps Per Scheduler          warp         7.75
    Eligible Warps Per Scheduler        warp         1.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.51%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.75 active warps per scheduler, but only an average of 1.06 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.70
    Warp Cycles Per Executed Instruction           cycle        15.72
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.51%                                                                                          
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 52.9% of the total average of 15.7 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.34
    Issued Instructions                             inst      9404909
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.91
    Achieved Active Warps Per SM           warp        30.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.51%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229458
    Total DRAM Elapsed Cycles        cycle      2719744
    Average L1 Active Cycles         cycle     60795.34
    Total L1 Elapsed Cycles          cycle      5552420
    Average L2 Active Cycles         cycle     56127.69
    Total L2 Elapsed Cycles          cycle      1887712
    Average SM Active Cycles         cycle     60795.34
    Total SM Elapsed Cycles          cycle      5552420
    Average SMSP Active Cycles       cycle     59486.80
    Total SMSP Elapsed Cycles        cycle     22209680
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.919%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 7.90% above the average, while the minimum instance value is 6.69% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.771%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.07% above the average, while the minimum instance value is 7.95% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.919%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 7.90% above the average, while the minimum instance value is 6.69% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       252414
    Memory Throughput                 %        85.28
    DRAM Throughput                   %        85.28
    Duration                         us       110.02
    L1/TEX Cache Throughput           %        32.52
    L2 Cache Throughput               %        43.64
    SM Active Cycles              cycle    229372.60
    Compute (SM) Throughput           %        22.72
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.43
    Executed Ipc Elapsed  inst/cycle         0.39
    Issue Slots Busy               %        10.87
    Issued Ipc Active     inst/cycle         0.43
    SM Busy                        %        25.00
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.0%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       613.29
    Mem Busy                               %        43.64
    Max Bandwidth                          %        85.28
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.52
    Mem Pipes Busy                         %        15.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.05%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.38
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.62
    Active Warps Per Scheduler          warp         1.94
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 14.72%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.94 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.02
    Warp Cycles Per Executed Instruction           cycle        17.03
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.72%                                                                                          
          On average, each warp of this workload spends 6.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 39.3% of the total average of 17.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.57
    Issued Instructions                             inst      7979382
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9015%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.64
    Achieved Active Warps Per SM           warp         7.51
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.72%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1054240
    Total DRAM Elapsed Cycles        cycle      9889792
    Average L1 Active Cycles         cycle    229372.60
    Total L1 Elapsed Cycles          cycle     20188528
    Average L2 Active Cycles         cycle    197067.25
    Total L2 Elapsed Cycles          cycle      6862848
    Average SM Active Cycles         cycle    229372.60
    Total SM Elapsed Cycles          cycle     20188528
    Average SMSP Active Cycles       cycle    219207.03
    Total SMSP Elapsed Cycles        cycle     80754112
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.723%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.40% above the average, while the minimum instance value is 14.52% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.985%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.04% above the average, while the minimum instance value is 14.15% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.723%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.40% above the average, while the minimum instance value is 14.52% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03216%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5988144 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.07776%                                                                                        
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5254992 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.20
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        27947
    Memory Throughput                 %        48.56
    DRAM Throughput                   %        48.56
    Duration                         us        12.26
    L1/TEX Cache Throughput           %        23.42
    L2 Cache Throughput               %        35.09
    SM Active Cycles              cycle     24312.78
    Compute (SM) Throughput           %        26.34
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.18
    Executed Ipc Elapsed  inst/cycle         1.02
    Issue Slots Busy               %        30.24
    Issued Ipc Active     inst/cycle         1.21
    SM Busy                        %        30.24
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (24.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       347.95
    Mem Busy                               %        22.73
    Max Bandwidth                          %        48.56
    L1/TEX Hit Rate                        %        38.32
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.52
    Mem Pipes Busy                         %         5.87
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.57%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        30.37
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        69.63
    Active Warps Per Scheduler          warp         9.36
    Eligible Warps Per Scheduler        warp         0.71
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.44%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.36 active warps per scheduler, but only an average of 0.71 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        30.83
    Warp Cycles Per Executed Instruction           cycle        31.73
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.1%                                                                                           
          On average, each warp of this workload spends 12.7 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.1% of the total average of 30.8 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7351.37
    Issued Instructions                             inst      2352439
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.896%                                                                                          
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        75.50
    Achieved Active Warps Per SM           warp        36.24
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66632
    Total DRAM Elapsed Cycles        cycle      1097728
    Average L1 Active Cycles         cycle     24312.78
    Total L1 Elapsed Cycles          cycle      2232928
    Average L2 Active Cycles         cycle     18034.62
    Total L2 Elapsed Cycles          cycle       760256
    Average SM Active Cycles         cycle     24312.78
    Total SM Elapsed Cycles          cycle      2232928
    Average SMSP Active Cycles       cycle     24207.64
    Total SMSP Elapsed Cycles        cycle      8931712
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.95%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (131072, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       761227
    Memory Throughput                 %        92.86
    DRAM Throughput                   %        92.86
    Duration                         us       331.74
    L1/TEX Cache Throughput           %        27.55
    L2 Cache Throughput               %        40.54
    SM Active Cycles              cycle    753984.80
    Compute (SM) Throughput           %         3.66
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.15
    Executed Ipc Elapsed  inst/cycle         0.15
    Issue Slots Busy               %         3.70
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %         3.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.83%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       667.91
    Mem Busy                               %        38.77
    Max Bandwidth                          %        92.86
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        99.99
    Mem Pipes Busy                         %         3.44
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.71
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.29
    Active Warps Per Scheduler          warp        10.66
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.139%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 26.9 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.66 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       287.08
    Warp Cycles Per Executed Instruction           cycle       287.48
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.139%                                                                                          
          On average, each warp of this workload spends 118.0 cycles being stalled after EXIT waiting for all           
          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  
          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  
          sure the memory access patterns of these store operations are optimal for the target architecture and         
          consider parallelized data reduction, if applicable. This stall type represents about 41.1% of the total      
          average of 287.1 cycles between issuing two instructions.                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     27852.80
    Executed Instructions                           inst      8912896
    Avg. Issued Instructions Per Scheduler          inst     27891.43
    Issued Instructions                             inst      8925257
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        16777216
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              136.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.85
    Achieved Active Warps Per SM           warp        42.65
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.139%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.8%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3462088
    Total DRAM Elapsed Cycles        cycle     29826048
    Average L1 Active Cycles         cycle    753984.80
    Total L1 Elapsed Cycles          cycle     60897460
    Average L2 Active Cycles         cycle    643665.38
    Total L2 Elapsed Cycles          cycle     20697120
    Average SM Active Cycles         cycle    753984.80
    Total SM Elapsed Cycles          cycle     60897460
    Average SMSP Active Cycles       cycle    751472.90
    Total SMSP Elapsed Cycles        cycle    243589840
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst      1048576
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        10.72
    SM Frequency                    Ghz         2.20
    Elapsed Cycles                cycle         3528
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.29
    Duration                         us         1.60
    L1/TEX Cache Throughput           %        39.77
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        30.18
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.08
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.61
    Issued Ipc Active     inst/cycle         0.10
    SM Busy                        %         2.61
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.59%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s            2
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.46
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.56
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.73
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.27
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.27%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.6 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.56
    Warp Cycles Per Executed Instruction           cycle        45.16
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 76.68%                                                                                          
          On average, each warp of this workload spends 28.0 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 76.7% of the total average of 36.6 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         8.06
    Achieved Active Warps Per SM           warp         3.87
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.94%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (8.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           50
    Total DRAM Elapsed Cycles        cycle       137216
    Average L1 Active Cycles         cycle        30.18
    Total L1 Elapsed Cycles          cycle       282132
    Average L2 Active Cycles         cycle       149.06
    Total L2 Elapsed Cycles          cycle        95808
    Average SM Active Cycles         cycle        30.18
    Total SM Elapsed Cycles          cycle       282132
    Average SMSP Active Cycles       cycle        28.81
    Total SMSP Elapsed Cycles        cycle      1128528
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.13
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        15082
    Memory Throughput                 %         0.49
    DRAM Throughput                   %         0.39
    Duration                         us         6.62
    L1/TEX Cache Throughput           %        11.61
    L2 Cache Throughput               %         0.49
    SM Active Cycles              cycle       158.75
    Compute (SM) Throughput           %         0.04
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.24
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.24
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.18%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.76
    Mem Busy                               %         0.49
    Max Bandwidth                          %         0.41
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.40
    Mem Pipes Busy                         %         0.04
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.13%                                                                                           
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1263%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.968%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.122%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         9.01
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        90.99
    Active Warps Per Scheduler          warp         1.02
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 90.99%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.02 active warps per scheduler, but only an average of 0.09 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.34
    Warp Cycles Per Executed Instruction           cycle        11.46
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.04%                                                                                          
          On average, each warp of this workload spends 4.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.0% of the total average of 11.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01395%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 90.99%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       589824
    Average L1 Active Cycles         cycle       158.75
    Total L1 Elapsed Cycles          cycle      1206432
    Average L2 Active Cycles         cycle       736.94
    Total L2 Elapsed Cycles          cycle       410144
    Average SM Active Cycles         cycle       158.75
    Total SM Elapsed Cycles          cycle      1206432
    Average SMSP Active Cycles       cycle        39.50
    Total SMSP Elapsed Cycles        cycle      4825728
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.982%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.5578%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.23
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       154046
    Memory Throughput                 %        74.38
    DRAM Throughput                   %        74.38
    Duration                         us        67.20
    L1/TEX Cache Throughput           %        36.99
    L2 Cache Throughput               %        40.96
    SM Active Cycles              cycle    115196.24
    Compute (SM) Throughput           %        21.29
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.41
    Issue Slots Busy               %        13.57
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.45
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       534.75
    Mem Busy                               %        40.96
    Max Bandwidth                          %        74.38
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        14.71
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 33.48%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.64
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.36
    Active Warps Per Scheduler          warp         1.75
    Eligible Warps Per Scheduler        warp         0.15
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.62%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.75 active warps per scheduler, but only an average of 0.15 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.86
    Warp Cycles Per Executed Instruction           cycle        12.87
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.62%                                                                                          
          On average, each warp of this workload spends 4.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 34.0% of the total average of 12.9 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.42
    Issued Instructions                             inst      5003015
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.73
    Achieved Active Warps Per SM           warp         7.07
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.62%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       561492
    Total DRAM Elapsed Cycles        cycle      6039552
    Average L1 Active Cycles         cycle    115196.24
    Total L1 Elapsed Cycles          cycle     12311048
    Average L2 Active Cycles         cycle    117531.06
    Total L2 Elapsed Cycles          cycle      4188480
    Average SM Active Cycles         cycle    115196.24
    Total SM Elapsed Cycles          cycle     12311048
    Average SMSP Active Cycles       cycle    114629.30
    Total SMSP Elapsed Cycles        cycle     49244192
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 10.64%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.22% above the average, while the minimum instance value is 18.92% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.71%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 15.72% above the average, while the minimum instance value is 18.77% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 10.64%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.22% above the average, while the minimum instance value is 18.92% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1108%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038576 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69819
    Memory Throughput                 %        67.14
    DRAM Throughput                   %        67.14
    Duration                         us        30.46
    L1/TEX Cache Throughput           %        14.80
    L2 Cache Throughput               %        24.18
    SM Active Cycles              cycle     60867.46
    Compute (SM) Throughput           %        42.11
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.93
    Executed Ipc Elapsed  inst/cycle         1.68
    Issue Slots Busy               %        48.29
    Issued Ipc Active     inst/cycle         1.93
    SM Busy                        %        48.29
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       482.05
    Mem Busy                               %        18.21
    Max Bandwidth                          %        67.14
    L1/TEX Hit Rate                        %        15.30
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.91
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        49.45
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        50.55
    Active Warps Per Scheduler          warp         7.72
    Eligible Warps Per Scheduler        warp         1.05
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.86%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.72 active warps per scheduler, but only an average of 1.05 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.61
    Warp Cycles Per Executed Instruction           cycle        15.63
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.86%                                                                                          
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.2% of the total average of 15.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.38
    Issued Instructions                             inst      9404920
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.48
    Achieved Active Warps Per SM           warp        29.99
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.86%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229458
    Total DRAM Elapsed Cycles        cycle      2734080
    Average L1 Active Cycles         cycle     60867.46
    Total L1 Elapsed Cycles          cycle      5583396
    Average L2 Active Cycles         cycle     56021.72
    Total L2 Elapsed Cycles          cycle      1898016
    Average SM Active Cycles         cycle     60867.46
    Total SM Elapsed Cycles          cycle      5583396
    Average SMSP Active Cycles       cycle     59430.60
    Total SMSP Elapsed Cycles        cycle     22333584
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.782%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles. Maximum  
          instance value is 8.92% above the average, while the minimum instance value is 6.80% below the average.       
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.41%                                                                                           
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 8.70% above the average, while the minimum instance value is 7.63% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.782%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Maximum instance value is 8.92% above the average, while the minimum instance value is 6.80% below the        
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       248868
    Memory Throughput                 %        86.47
    DRAM Throughput                   %        86.47
    Duration                         us       108.48
    L1/TEX Cache Throughput           %        32.59
    L2 Cache Throughput               %        44.26
    SM Active Cycles              cycle    228871.81
    Compute (SM) Throughput           %        23.05
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.44
    Executed Ipc Elapsed  inst/cycle         0.40
    Issue Slots Busy               %        10.89
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.06
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       621.85
    Mem Busy                               %        44.26
    Max Bandwidth                          %        86.47
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.53
    Mem Pipes Busy                         %        15.25
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.51%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.32
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.68
    Active Warps Per Scheduler          warp         1.95
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.95 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.24
    Warp Cycles Per Executed Instruction           cycle        17.25
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.53%                                                                                          
          On average, each warp of this workload spends 6.7 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 39.1% of the total average of 17.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.52
    Issued Instructions                             inst      7979365
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9035%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.64
    Achieved Active Warps Per SM           warp         7.51
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.53%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1054034
    Total DRAM Elapsed Cycles        cycle      9751552
    Average L1 Active Cycles         cycle    228871.81
    Total L1 Elapsed Cycles          cycle     19904624
    Average L2 Active Cycles         cycle    195998.72
    Total L2 Elapsed Cycles          cycle      6766304
    Average SM Active Cycles         cycle    228871.81
    Total SM Elapsed Cycles          cycle     19904624
    Average SMSP Active Cycles       cycle    220234.20
    Total SMSP Elapsed Cycles        cycle     79618496
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 5.889%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.40% above the average, while the minimum instance value is 14.38% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.041%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 6.82% above the average, while the minimum instance value is 13.18% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 5.889%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.40% above the average, while the minimum instance value is 14.38% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03245%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5988048 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.0787%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5255014 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.19
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        28355
    Memory Throughput                 %        47.98
    DRAM Throughput                   %        47.98
    Duration                         us        12.42
    L1/TEX Cache Throughput           %        23.42
    L2 Cache Throughput               %        34.48
    SM Active Cycles              cycle     24927.53
    Compute (SM) Throughput           %        25.97
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.15
    Executed Ipc Elapsed  inst/cycle         1.01
    Issue Slots Busy               %        29.49
    Issued Ipc Active     inst/cycle         1.18
    SM Busy                        %        29.49
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (23.4%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       343.45
    Mem Busy                               %        22.35
    Max Bandwidth                          %        47.98
    L1/TEX Hit Rate                        %        38.71
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.43
    Mem Pipes Busy                         %         5.79
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.56%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        30.51
    Issued Warp Per Scheduler                        0.31
    No Eligible                            %        69.49
    Active Warps Per Scheduler          warp         8.95
    Eligible Warps Per Scheduler        warp         0.72
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.02%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.3 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.95 active warps per scheduler, but only an average of 0.72 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.32
    Warp Cycles Per Executed Instruction           cycle        30.18
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 44.43%                                                                                          
          On average, each warp of this workload spends 13.0 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 44.4% of the total average of 29.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7351.30
    Issued Instructions                             inst      2352417
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.751%                                                                                          
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        72.75
    Achieved Active Warps Per SM           warp        34.92
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 12.7%                                                                                           
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (72.8%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66630
    Total DRAM Elapsed Cycles        cycle      1111040
    Average L1 Active Cycles         cycle     24927.53
    Total L1 Elapsed Cycles          cycle      2264872
    Average L2 Active Cycles         cycle     18227.75
    Total L2 Elapsed Cycles          cycle       770688
    Average SM Active Cycles         cycle     24927.53
    Total SM Elapsed Cycles          cycle      2264872
    Average SMSP Active Cycles       cycle     24094.15
    Total SMSP Elapsed Cycles        cycle      9059488
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.84%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        10.72
    SM Frequency                    Ghz         2.20
    Elapsed Cycles                cycle         3518
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.30
    Duration                         us         1.60
    L1/TEX Cache Throughput           %        40.52
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        29.61
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.66
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %         2.66
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.56%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.08
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.46
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.82
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.75
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.25
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.25%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 36.4 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, but only an average of 0.03 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, reduce the time the active warps are stalled by inspecting the top stall reasons    
          on the Warp State Statistics and Source Counters sections.                                                    

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.49
    Warp Cycles Per Executed Instruction           cycle        45.08
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 75.38%                                                                                          
          On average, each warp of this workload spends 27.5 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 75.4% of the total average of 36.5 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         8.21
    Achieved Active Warps Per SM           warp         3.94
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.79%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (8.2%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           52
    Total DRAM Elapsed Cycles        cycle       137216
    Average L1 Active Cycles         cycle        29.61
    Total L1 Elapsed Cycles          cycle       281168
    Average L2 Active Cycles         cycle       147.97
    Total L2 Elapsed Cycles          cycle        95552
    Average SM Active Cycles         cycle        29.61
    Total SM Elapsed Cycles          cycle       281168
    Average SMSP Active Cycles       cycle        28.66
    Total SMSP Elapsed Cycles        cycle      1124672
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.21
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        17137
    Memory Throughput                 %         0.43
    DRAM Throughput                   %         0.34
    Duration                         us         7.49
    L1/TEX Cache Throughput           %        11.56
    L2 Cache Throughput               %         0.43
    SM Active Cycles              cycle       159.43
    Compute (SM) Throughput           %         0.03
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.23
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.23
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.18%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.44
    Mem Busy                               %         0.43
    Max Bandwidth                          %         0.36
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.49
    Mem Pipes Busy                         %         0.03
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.1144%                                                                                         
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.1111%                                                                                         
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.947%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.092%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.99
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.01
    Active Warps Per Scheduler          warp         1.00
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.01%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.00 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.12
    Warp Cycles Per Executed Instruction           cycle        11.23
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.62%                                                                                          
          On average, each warp of this workload spends 4.6 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.6% of the total average of 11.1 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.01228%                                                                                        
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.10
    Achieved Active Warps Per SM           warp         1.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.01%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.1%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       671744
    Average L1 Active Cycles         cycle       159.43
    Total L1 Elapsed Cycles          cycle      1370684
    Average L2 Active Cycles         cycle       716.56
    Total L2 Elapsed Cycles          cycle       465888
    Average SM Active Cycles         cycle       159.43
    Total SM Elapsed Cycles          cycle      1370684
    Average SMSP Active Cycles       cycle        39.58
    Total SMSP Elapsed Cycles        cycle      5482736
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 4.264%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.493%                                                                                          
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       148211
    Memory Throughput                 %        77.35
    DRAM Throughput                   %        77.35
    Duration                         us        64.64
    L1/TEX Cache Throughput           %        36.59
    L2 Cache Throughput               %        42.57
    SM Active Cycles              cycle    116468.77
    Compute (SM) Throughput           %        22.13
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.42
    Issue Slots Busy               %        13.42
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.13
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.1%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       556.27
    Mem Busy                               %        42.57
    Max Bandwidth                          %        77.35
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        15.28
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 34.8%                                                                                           
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.55
    Issued Warp Per Scheduler                        0.14
    No Eligible                            %        86.45
    Active Warps Per Scheduler          warp         1.76
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 22.65%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.76 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.99
    Warp Cycles Per Executed Instruction           cycle        13.01
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 22.65%                                                                                          
          On average, each warp of this workload spends 4.5 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 34.4% of the total average of 13.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.39
    Issued Instructions                             inst      5003005
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.60
    Achieved Active Warps Per SM           warp         7.01
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 22.65%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       561836
    Total DRAM Elapsed Cycles        cycle      5811200
    Average L1 Active Cycles         cycle    116468.77
    Total L1 Elapsed Cycles          cycle     11845744
    Average L2 Active Cycles         cycle    117665.84
    Total L2 Elapsed Cycles          cycle      4029920
    Average SM Active Cycles         cycle    116468.77
    Total SM Elapsed Cycles          cycle     11845744
    Average SMSP Active Cycles       cycle    115356.32
    Total SMSP Elapsed Cycles        cycle     47382976
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 11.19%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 14.22% above the average, while the minimum instance value is 19.68% below the average.     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.12%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 15.56% above the average, while the minimum instance value is 16.51% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 11.19%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 14.22% above the average, while the minimum instance value is 19.68% below the      
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1164%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038574 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69766
    Memory Throughput                 %        67.19
    DRAM Throughput                   %        67.19
    Duration                         us        30.43
    L1/TEX Cache Throughput           %        14.80
    L2 Cache Throughput               %        24.20
    SM Active Cycles              cycle     60891.55
    Compute (SM) Throughput           %        42.15
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.93
    Executed Ipc Elapsed  inst/cycle         1.68
    Issue Slots Busy               %        48.27
    Issued Ipc Active     inst/cycle         1.93
    SM Busy                        %        48.27
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.5%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       482.57
    Mem Busy                               %        18.22
    Max Bandwidth                          %        67.19
    L1/TEX Hit Rate                        %        15.29
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.92
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        48.87
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        51.13
    Active Warps Per Scheduler          warp         7.57
    Eligible Warps Per Scheduler        warp         1.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.81%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.57 active warps per scheduler, but only an average of 1.04 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.50
    Warp Cycles Per Executed Instruction           cycle        15.52
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.81%                                                                                          
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.5% of the total average of 15.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.38
    Issued Instructions                             inst      9404920
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.80
    Achieved Active Warps Per SM           warp        30.14
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.81%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229460
    Total DRAM Elapsed Cycles        cycle      2732032
    Average L1 Active Cycles         cycle     60891.55
    Total L1 Elapsed Cycles          cycle      5578732
    Average L2 Active Cycles         cycle     55901.28
    Total L2 Elapsed Cycles          cycle      1896640
    Average SM Active Cycles         cycle     60891.55
    Total SM Elapsed Cycles          cycle      5578732
    Average SMSP Active Cycles       cycle     60141.96
    Total SMSP Elapsed Cycles        cycle     22314928
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.229%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.28% above the average, while the minimum instance value is 6.55% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.902%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 9.16% above the average, while the minimum instance value is 7.87% below    
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.229%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.28% above the average, while the minimum instance value is 6.55% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       248431
    Memory Throughput                 %        86.64
    DRAM Throughput                   %        86.64
    Duration                         us       108.29
    L1/TEX Cache Throughput           %        32.89
    L2 Cache Throughput               %        44.34
    SM Active Cycles              cycle    226799.35
    Compute (SM) Throughput           %        23.09
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.44
    Executed Ipc Elapsed  inst/cycle         0.40
    Issue Slots Busy               %        10.99
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.28
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.3%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       623.02
    Mem Busy                               %        44.34
    Max Bandwidth                          %        86.64
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.53
    Mem Pipes Busy                         %        15.28
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.56%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.44
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.56
    Active Warps Per Scheduler          warp         1.99
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.36%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.7 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.99 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.36
    Warp Cycles Per Executed Instruction           cycle        17.38
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.36%                                                                                          
          On average, each warp of this workload spends 6.9 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 39.7% of the total average of 17.4 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.53
    Issued Instructions                             inst      7979370
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9117%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.61
    Achieved Active Warps Per SM           warp         7.49
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.36%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1054156
    Total DRAM Elapsed Cycles        cycle      9734144
    Average L1 Active Cycles         cycle    226799.35
    Total L1 Elapsed Cycles          cycle     19869280
    Average L2 Active Cycles         cycle    196585.72
    Total L2 Elapsed Cycles          cycle      6754432
    Average SM Active Cycles         cycle    226799.35
    Total SM Elapsed Cycles          cycle     19869280
    Average SMSP Active Cycles       cycle    217993.47
    Total SMSP Elapsed Cycles        cycle     79477120
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.274%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 6.87% above the average, while the minimum instance value is 15.34% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.093%                                                                                          
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.08% above the average, while the minimum instance value is 10.37% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.274%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 6.87% above the average, while the minimum instance value is 15.34% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.0326%                                                                                         
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5987408 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.07813%                                                                                        
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5254998 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.17
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        28506
    Memory Throughput                 %        47.67
    DRAM Throughput                   %        47.67
    Duration                         us        12.51
    L1/TEX Cache Throughput           %        22.93
    L2 Cache Throughput               %        34.28
    SM Active Cycles              cycle     25105.11
    Compute (SM) Throughput           %        25.82
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.14
    Executed Ipc Elapsed  inst/cycle         1.00
    Issue Slots Busy               %        29.28
    Issued Ipc Active     inst/cycle         1.17
    SM Busy                        %        29.28
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (23.2%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       340.82
    Mem Busy                               %        22.34
    Max Bandwidth                          %        47.67
    L1/TEX Hit Rate                        %        38.33
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.66
    Mem Pipes Busy                         %         5.75
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.2%                                                                                           
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        28.94
    Issued Warp Per Scheduler                        0.29
    No Eligible                            %        71.06
    Active Warps Per Scheduler          warp         8.53
    Eligible Warps Per Scheduler        warp         0.67
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 52.33%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.5 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 8.53 active warps per scheduler, but only an average of 0.67 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        29.49
    Warp Cycles Per Executed Instruction           cycle        30.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 42.81%                                                                                          
          On average, each warp of this workload spends 12.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 42.8% of the total average of 29.5 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7351.37
    Issued Instructions                             inst      2352439
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.71%                                                                                           
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        71.50
    Achieved Active Warps Per SM           warp        34.32
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 14.19%                                                                                          
          The difference between calculated theoretical (83.3%) and measured achieved occupancy (71.5%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66630
    Total DRAM Elapsed Cycles        cycle      1118208
    Average L1 Active Cycles         cycle     25105.11
    Total L1 Elapsed Cycles          cycle      2277876
    Average L2 Active Cycles         cycle     18071.59
    Total L2 Elapsed Cycles          cycle       775552
    Average SM Active Cycles         cycle     25105.11
    Total SM Elapsed Cycles          cycle      2277876
    Average SMSP Active Cycles       cycle     25401.33
    Total SMSP Elapsed Cycles        cycle      9111504
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.28%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (131072, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       762099
    Memory Throughput                 %        92.89
    DRAM Throughput                   %        92.89
    Duration                         us       332.10
    L1/TEX Cache Throughput           %        27.52
    L2 Cache Throughput               %        41.76
    SM Active Cycles              cycle    753893.79
    Compute (SM) Throughput           %         3.66
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.15
    Executed Ipc Elapsed  inst/cycle         0.15
    Issue Slots Busy               %         3.70
    Issued Ipc Active     inst/cycle         0.15
    SM Busy                        %         3.70
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.83%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       668.16
    Mem Busy                               %        41.76
    Max Bandwidth                          %        92.89
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %       100.00
    Mem Pipes Busy                         %         3.44
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         3.71
    Issued Warp Per Scheduler                        0.04
    No Eligible                            %        96.29
    Active Warps Per Scheduler          warp        10.63
    Eligible Warps Per Scheduler        warp         0.06
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 7.111%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 27.0 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 10.63 active warps per scheduler, but only an average of 0.06 warps were eligible per cycle. Eligible      
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle       286.96
    Warp Cycles Per Executed Instruction           cycle       287.35
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    30.12
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.111%                                                                                          
          On average, each warp of this workload spends 117.9 cycles being stalled after EXIT waiting for all           
          outstanding memory operations to complete so that warp's resources can be freed. A high number of stalls due  
          to draining warps typically occurs when a lot of data is written to memory towards the end of a kernel. Make  
          sure the memory access patterns of these store operations are optimal for the target architecture and         
          consider parallelized data reduction, if applicable. This stall type represents about 41.1% of the total      
          average of 287.0 cycles between issuing two instructions.                                                     
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     27852.80
    Executed Instructions                           inst      8912896
    Avg. Issued Instructions Per Scheduler          inst     27891.39
    Issued Instructions                             inst      8925244
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                 131072
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread        16777216
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                              136.53
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %        88.46
    Achieved Active Warps Per SM           warp        42.46
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 7.111%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (88.5%) can be the     
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      3467060
    Total DRAM Elapsed Cycles        cycle     29859840
    Average L1 Active Cycles         cycle    753893.79
    Total L1 Elapsed Cycles          cycle     60967284
    Average L2 Active Cycles         cycle    642879.56
    Total L2 Elapsed Cycles          cycle     20720992
    Average SM Active Cycles         cycle    753893.79
    Total SM Elapsed Cycles          cycle     60967284
    Average SMSP Active Cycles       cycle    752594.33
    Total SMSP Elapsed Cycles        cycle    243869136
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.12
    Branch Instructions              inst      1048576
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  void at::vectorized_elementwise_kernel<4, at::FillFunctor<int>, std::array<char *, 1>>(int, T2, T3) (2, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.02
    SM Frequency                    Ghz         2.26
    Elapsed Cycles                cycle         3542
    Memory Throughput                 %         0.61
    DRAM Throughput                   %         0.30
    Duration                         us         1.57
    L1/TEX Cache Throughput           %        40.12
    L2 Cache Throughput               %         0.61
    SM Active Cycles              cycle        29.91
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    WRN   Sampling interval is larger than 10% of the workload duration, which likely results in very few collected     
          samples.                                                                                                      

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.09
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.63
    Issued Ipc Active     inst/cycle         0.11
    SM Busy                        %         2.63
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.58%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         2.12
    Mem Busy                               %         0.61
    Max Bandwidth                          %         0.45
    L1/TEX Hit Rate                        %            0
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        89.81
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         2.70
    Issued Warp Per Scheduler                        0.03
    No Eligible                            %        97.30
    Active Warps Per Scheduler          warp         0.98
    Eligible Warps Per Scheduler        warp         0.03
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 97.3%                                                                                     
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 37.1 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 0.98 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        36.27
    Warp Cycles Per Executed Instruction           cycle        44.80
    Avg. Active Threads Per Warp                                31.84
    Avg. Not Predicated Off Threads Per Warp                    28.75
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 77.68%                                                                                          
          On average, each warp of this workload spends 28.2 cycles being stalled waiting for an immediate constant     
          cache (IMC) miss. A read from constant memory costs one memory read from device memory only on a cache miss;  
          otherwise, it just costs one read from the constant cache. Immediate constants are encoded into the SASS      
          instruction as 'c[bank][offset]'. Accesses to different addresses by threads within a warp are serialized,    
          thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As       
          such, the constant cache is best when threads in the same warp access only a few distinct locations. If all   
          threads of a warp access the same location, then constant memory can be as fast as a register access. This    
          stall type represents about 77.7% of the total average of 36.3 cycles between issuing two instructions.       
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         0.64
    Executed Instructions                           inst          204
    Avg. Issued Instructions Per Scheduler          inst         0.79
    Issued Instructions                             inst          252
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      2
    Registers Per Thread             register/thread              16
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread             256
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 97.5%                                                                                           
          The grid for this launch is configured to execute only 2 blocks, which is less than the GPU's 80              
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           32
    Block Limit Shared Mem                block           32
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           48
    Theoretical Occupancy                     %          100
    Achieved Occupancy                        %         8.14
    Achieved Active Warps Per SM           warp         3.91
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.86%                                                                                          
          The difference between calculated theoretical (100.0%) and measured achieved occupancy (8.1%) can be the      
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle           52
    Total DRAM Elapsed Cycles        cycle       138240
    Average L1 Active Cycles         cycle        29.91
    Total L1 Elapsed Cycles          cycle       282932
    Average L2 Active Cycles         cycle       148.91
    Total L2 Elapsed Cycles          cycle        96224
    Average SM Active Cycles         cycle        29.91
    Total SM Elapsed Cycles          cycle       282932
    Average SMSP Active Cycles       cycle        29.20
    Total SMSP Elapsed Cycles        cycle      1131728
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst           32
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    WRN   Sampling metrics were enabled, but no samples could be collected for this kernel.                             

  void moe::moe_align_block_size_kernel<long, int>(T1 *, int *, int *, int *, int, int, unsigned long) (1, 1, 1)x(32, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.20
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        23454
    Memory Throughput                 %         0.32
    DRAM Throughput                   %         0.25
    Duration                         us        10.27
    L1/TEX Cache Throughput           %        11.09
    L2 Cache Throughput               %         0.32
    SM Active Cycles              cycle       166.30
    Compute (SM) Throughput           %         0.02
    ----------------------- ----------- ------------

    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      
          waves across all SMs. Look at Launch Statistics for more details.                                             

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.08
    Executed Ipc Elapsed  inst/cycle         0.00
    Issue Slots Busy               %         2.14
    Issued Ipc Active     inst/cycle         0.09
    SM Busy                        %         2.14
    -------------------- ----------- ------------

    OPT   Est. Local Speedup: 98.26%                                                                                    
          All compute pipelines are under-utilized. Either this workload is very small or it doesn't issue enough warps 
          per scheduler. Check the Launch Statistics and Scheduler Statistics sections for further details.             

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s         1.78
    Mem Busy                               %         0.32
    Max Bandwidth                          %         0.27
    L1/TEX Hit Rate                        %        88.77
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        85.49
    Mem Pipes Busy                         %         0.02
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 0.08358%                                                                                        
          The memory access pattern for global loads from L1TEX might not be optimal. On average, only 4.0 of the 32    
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global loads.                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.08119%                                                                                        
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 4.8 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 4.742%                                                                                          
          The memory access pattern for shared loads might not be optimal and causes on average a 1.7 - way bank        
          conflict across all 99 shared load requests.This results in 74 bank conflicts,  which represent 42.77% of     
          the overall 173 wavefronts for shared loads. Check the Source Counters section for uncoalesced shared loads.  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.799%                                                                                          
          The memory access pattern for shared stores might not be optimal and causes on average a 2.6 - way bank       
          conflict across all 82 shared store requests.This results in 130 bank conflicts,  which represent 61.32% of   
          the overall 212 wavefronts for shared stores. Check the Source Counters section for uncoalesced shared        
          stores.                                                                                                       

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %         8.87
    Issued Warp Per Scheduler                        0.09
    No Eligible                            %        91.13
    Active Warps Per Scheduler          warp         0.99
    Eligible Warps Per Scheduler        warp         0.09
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 91.13%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 11.3 cycles. This might leave hardware resources underutilized and may lead to    
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 0.99 active warps per scheduler, which already limits the scheduler to less than a warp per instruction.   

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        11.19
    Warp Cycles Per Executed Instruction           cycle        11.31
    Avg. Active Threads Per Warp                                20.92
    Avg. Not Predicated Off Threads Per Warp                    20.44
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.46%                                                                                          
          On average, each warp of this workload spends 4.6 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.5% of the total average of 11.2 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.008973%                                                                                       
          Instructions are executed in warps, which are groups of 32 threads. Optimal instruction throughput is         
          achieved if all 32 threads of a warp execute the same instruction. The chosen launch configuration, early     
          thread completion, and divergent flow control can significantly lower the number of active threads in a warp  
          per cycle. This workload achieves an average of 20.9 threads being active per cycle. This is further reduced  
          to 20.4 threads per warp due to predication. The compiler may use predication to avoid an actual branch.      
          Instead, all instructions are scheduled, but a per-thread condition code or predicate controls which threads  
          execute the instructions. Try to avoid different execution paths within a warp when possible.                 

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst         3.52
    Executed Instructions                           inst         1127
    Avg. Issued Instructions Per Scheduler          inst         3.56
    Issued Instructions                             inst         1139
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                    32
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                      1
    Registers Per Thread             register/thread              27
    Shared Memory Configuration Size           Kbyte           65.54
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block            1.09
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread              32
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                0.00
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 98.75%                                                                                          
          The grid for this launch is configured to execute only 1 block, which is less than the GPU's 80               
          multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel      
          concurrently with other workloads, consider reducing the block size to have at least one block per            
          multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the    
          Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model)            
          description for more details on launch configurations.                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           64
    Block Limit Shared Mem                block           30
    Block Limit Warps                     block           48
    Theoretical Active Warps per SM        warp           24
    Theoretical Occupancy                     %           50
    Achieved Occupancy                        %         2.00
    Achieved Active Warps Per SM           warp         0.96
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 91.13%                                                                                          
          The difference between calculated theoretical (50.0%) and measured achieved occupancy (2.0%) can be the       
          result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can   
          occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices   
          Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on     
          optimizing occupancy.                                                                                         
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 50%                                                                                             
          The 6.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (50.0%) is limited by the number of blocks that   
          can fit on the SM.                                                                                            

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle          286
    Total DRAM Elapsed Cycles        cycle       920576
    Average L1 Active Cycles         cycle       166.30
    Total L1 Elapsed Cycles          cycle      1876152
    Average L2 Active Cycles         cycle          791
    Total L2 Elapsed Cycles          cycle       637728
    Average SM Active Cycles         cycle       166.30
    Total SM Elapsed Cycles          cycle      1876152
    Average SMSP Active Cycles       cycle        40.14
    Total SMSP Elapsed Cycles        cycle      7504608
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.07
    Branch Instructions              inst           74
    Branch Efficiency                   %        93.65
    Avg. Divergent Branches                       0.01
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 3.439%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 1265 excessive sectors (87% of the total  
          1460 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations. The  
          CUDA Programming Guide                                                                                        
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.3757%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 204 excessive wavefronts (53% of the      
          total 385 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations. The   
          CUDA Best Practices Guide                                                                                     
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  fused_moe_kernel (7168, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       154318
    Memory Throughput                 %        74.29
    DRAM Throughput                   %        74.29
    Duration                         us        67.33
    L1/TEX Cache Throughput           %        36.94
    L2 Cache Throughput               %        40.88
    SM Active Cycles              cycle    115357.70
    Compute (SM) Throughput           %        21.26
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 0%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.54
    Executed Ipc Elapsed  inst/cycle         0.41
    Issue Slots Busy               %        13.55
    Issued Ipc Active     inst/cycle         0.54
    SM Busy                        %        28.41
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (28.4%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       534.23
    Mem Busy                               %        40.88
    Max Bandwidth                          %        74.29
    L1/TEX Hit Rate                        %         0.99
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        67.51
    Mem Pipes Busy                         %        14.68
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 33.42%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 5.6 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.0% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        13.49
    Issued Warp Per Scheduler                        0.13
    No Eligible                            %        86.51
    Active Warps Per Scheduler          warp         1.75
    Eligible Warps Per Scheduler        warp         0.14
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 25.71%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 7.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.75 active warps per scheduler, but only an average of 0.14 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        12.95
    Warp Cycles Per Executed Instruction           cycle        12.97
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    30.77
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.71%                                                                                          
          On average, each warp of this workload spends 4.4 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 34.2% of the total average of 13.0 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     15614.40
    Executed Instructions                           inst      4996608
    Avg. Issued Instructions Per Scheduler          inst     15634.40
    Issued Instructions                             inst      5003007
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   7168
    Registers Per Thread             register/thread             114
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          917504
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               44.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        14.71
    Achieved Active Warps Per SM           warp         7.06
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 25.71%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       562008
    Total DRAM Elapsed Cycles        cycle      6051840
    Average L1 Active Cycles         cycle    115357.70
    Total L1 Elapsed Cycles          cycle     12333176
    Average L2 Active Cycles         cycle       119598
    Total L2 Elapsed Cycles          cycle      4196416
    Average SM Active Cycles         cycle    115357.70
    Total SM Elapsed Cycles          cycle     12333176
    Average SMSP Active Cycles       cycle    115867.59
    Total SMSP Elapsed Cycles        cycle     49332704
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 12.37%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 16.54% above the average, while the minimum instance value is 17.40% below the      
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.77%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 17.00% above the average, while the minimum instance value is 18.52% below  
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 12.37%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 16.54% above the average, while the minimum instance value is 17.40% below  
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        50944
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.1107%                                                                                         
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 3038574 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void vllm::act_and_mul_kernel<c10::Half, &vllm::silu_kernel<c10::Half>, 1>(T1 *, const T1 *, int) (512, 1, 1)x(1024, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.22
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle        69648
    Memory Throughput                 %        67.27
    DRAM Throughput                   %        67.27
    Duration                         us        30.40
    L1/TEX Cache Throughput           %        14.83
    L2 Cache Throughput               %        24.25
    SM Active Cycles              cycle     60765.49
    Compute (SM) Throughput           %        42.22
    ----------------------- ----------- ------------

    OPT   Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the    
          DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the       
          bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or  
          whether there are values you can (re)compute.                                                                 

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved 6%  
          of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel Profiling Guide    
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details on roofline      
          analysis.                                                                                                     

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.93
    Executed Ipc Elapsed  inst/cycle         1.69
    Issue Slots Busy               %        48.37
    Issued Ipc Active     inst/cycle         1.93
    SM Busy                        %        48.37
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (37.6%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       483.07
    Mem Busy                               %        18.25
    Max Bandwidth                          %        67.27
    L1/TEX Hit Rate                        %        15.31
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        33.60
    Mem Pipes Busy                         %        12.94
    ---------------------------- ----------- ------------

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        48.97
    Issued Warp Per Scheduler                        0.49
    No Eligible                            %        51.03
    Active Warps Per Scheduler          warp         7.63
    Eligible Warps Per Scheduler        warp         1.04
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 32.73%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 2.0 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 7.63 active warps per scheduler, but only an average of 1.04 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        15.57
    Warp Cycles Per Executed Instruction           cycle        15.60
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.63
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.73%                                                                                          
          On average, each warp of this workload spends 8.3 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 53.5% of the total average of 15.6 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     29350.40
    Executed Instructions                           inst      9392128
    Avg. Issued Instructions Per Scheduler          inst     29390.37
    Issued Instructions                             inst      9404919
    ---------------------------------------- ----------- ------------

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                  1024
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                    512
    Registers Per Thread             register/thread              22
    Shared Memory Configuration Size           Kbyte            8.19
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          524288
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                6.40
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            2
    Block Limit Shared Mem                block            8
    Block Limit Warps                     block            1
    Theoretical Active Warps per SM        warp           32
    Theoretical Occupancy                     %        66.67
    Achieved Occupancy                        %        62.93
    Achieved Active Warps Per SM           warp        30.20
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 32.73%                                                                                          
          The 8.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (66.7%) is limited by the number of warps within  
          each block.                                                                                                   

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle       229458
    Total DRAM Elapsed Cycles        cycle      2728960
    Average L1 Active Cycles         cycle     60765.49
    Total L1 Elapsed Cycles          cycle      5569572
    Average L2 Active Cycles         cycle     55746.84
    Total L2 Elapsed Cycles          cycle      1893408
    Average SM Active Cycles         cycle     60765.49
    Total SM Elapsed Cycles          cycle      5569572
    Average SMSP Active Cycles       cycle     60018.67
    Total SMSP Elapsed Cycles        cycle     22278288
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 7.597%                                                                                          
          One or more SMs have a much higher number of active cycles than the average number of active cycles.          
          Additionally, other SMs have a much lower number of active cycles than the average number of active cycles.   
          Maximum instance value is 8.70% above the average, while the minimum instance value is 6.94% below the        
          average.                                                                                                      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 8.687%                                                                                          
          One or more SMSPs have a much higher number of active cycles than the average number of active cycles.        
          Additionally, other SMSPs have a much lower number of active cycles than the average number of active         
          cycles. Maximum instance value is 10.08% above the average, while the minimum instance value is 8.58% below   
          the average.                                                                                                  
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.597%                                                                                          
          One or more L1 Slices have a much higher number of active cycles than the average number of active cycles.    
          Additionally, other L1 Slices have a much lower number of active cycles than the average number of active     
          cycles. Maximum instance value is 8.70% above the average, while the minimum instance value is 6.94% below    
          the average.                                                                                                  

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.15
    Branch Instructions              inst      1372160
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

  fused_moe_kernel (2048, 1, 1)x(128, 1, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.24
    SM Frequency                    Ghz         2.29
    Elapsed Cycles                cycle       248328
    Memory Throughput                 %        86.65
    DRAM Throughput                   %        86.65
    Duration                         us       108.22
    L1/TEX Cache Throughput           %        32.75
    L2 Cache Throughput               %        44.36
    SM Active Cycles              cycle    227770.41
    Compute (SM) Throughput           %        23.10
    ----------------------- ----------- ------------

    INF   This workload is utilizing greater than 80.0% of the available compute or memory performance of the device.   
          To further improve performance, work will likely need to be shifted from the most utilized to another unit.   
          Start by analyzing DRAM in the Memory Workload Analysis section.                                              

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 0% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         0.44
    Executed Ipc Elapsed  inst/cycle         0.40
    Issue Slots Busy               %        10.95
    Issued Ipc Active     inst/cycle         0.44
    SM Busy                        %        25.18
    -------------------- ----------- ------------

    INF   Tensor is the highest-utilized pipeline (25.2%) based on active cycles, taking into account the rates of its  
          different instructions. It is the logical aggregation of individual tensor pipelines. It's dominated by its   
          Tensor (FP) sub-pipeline. It is well-utilized, but should not be a bottleneck.                                

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       623.26
    Mem Busy                               %        44.36
    Max Bandwidth                          %        86.65
    L1/TEX Hit Rate                        %         0.28
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        65.53
    Mem Pipes Busy                         %        15.28
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 32.58%                                                                                          
          The memory access pattern for global loads from L2 might not be optimal. On average, only 8.4 of the 32 bytes 
          transmitted per sector are utilized by each thread. This applies to the 99.7% of sectors missed in L1TEX.     
          This could possibly be caused by a stride between threads. Check the Source Counters section for uncoalesced  
          global loads.                                                                                                 

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        11.33
    Issued Warp Per Scheduler                        0.11
    No Eligible                            %        88.67
    Active Warps Per Scheduler          warp         1.96
    Eligible Warps Per Scheduler        warp         0.12
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 13.35%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 8.8 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 1.96 active warps per scheduler, but only an average of 0.12 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        17.27
    Warp Cycles Per Executed Instruction           cycle        17.28
    Avg. Active Threads Per Warp                                32.00
    Avg. Not Predicated Off Threads Per Warp                    31.37
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.35%                                                                                          
          On average, each warp of this workload spends 6.8 cycles being stalled waiting for a scoreboard dependency on 
          a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited     
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 39.6% of the total average of 17.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst     24916.40
    Executed Instructions                           inst      7973248
    Avg. Issued Instructions Per Scheduler          inst     24935.57
    Issued Instructions                             inst      7979381
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 0.9078%                                                                                         
          This kernel executes 0 fused and 10240 non-fused FP32 instructions. By converting pairs of non-fused          
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread             122
    Shared Memory Configuration Size           Kbyte          102.40
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block      Kbyte/block           49.15
    Static Shared Memory Per Block        byte/block               0
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                               12.80
    -------------------------------- --------------- ---------------

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block            4
    Block Limit Shared Mem                block            2
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp            8
    Theoretical Occupancy                     %        16.67
    Achieved Occupancy                        %        15.65
    Achieved Active Warps Per SM           warp         7.51
    ------------------------------- ----------- ------------

    OPT   Est. Speedup: 13.35%                                                                                          
          The 2.00 theoretical warps per scheduler this kernel can issue according to its occupancy are below the       
          hardware maximum of 12. This kernel's theoretical occupancy (16.7%) is limited by the required amount of      
          shared memory.                                                                                                

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle      1053938
    Total DRAM Elapsed Cycles        cycle      9730048
    Average L1 Active Cycles         cycle    227770.41
    Total L1 Elapsed Cycles          cycle     19861272
    Average L2 Active Cycles         cycle    198461.94
    Total L2 Elapsed Cycles          cycle      6751584
    Average SM Active Cycles         cycle    227770.41
    Total SM Elapsed Cycles          cycle     19861272
    Average SMSP Active Cycles       cycle    220006.77
    Total SMSP Elapsed Cycles        cycle     79445088
    -------------------------- ----------- ------------

    OPT   Est. Speedup: 6.583%                                                                                          
          One or more SMs have a much lower number of active cycles than the average number of active cycles. Maximum   
          instance value is 7.18% above the average, while the minimum instance value is 14.58% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 7.48%                                                                                           
          One or more SMSPs have a much lower number of active cycles than the average number of active cycles. Maximum 
          instance value is 8.44% above the average, while the minimum instance value is 15.49% below the average.      
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 6.583%                                                                                          
          One or more L1 Slices have a much lower number of active cycles than the average number of active cycles.     
          Maximum instance value is 7.18% above the average, while the minimum instance value is 14.58% below the       
          average.                                                                                                      

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.01
    Branch Instructions              inst        45824
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 0.03293%                                                                                        
          This kernel has uncoalesced global accesses resulting in a total of 2096 excessive sectors (0% of the total   
          5987560 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source locations.   
          The CUDA Programming Guide                                                                                    
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   
    ----- --------------------------------------------------------------------------------------------------------------
    OPT   Est. Speedup: 0.07849%                                                                                        
          This kernel has uncoalesced shared accesses resulting in a total of 4496 excessive wavefronts (0% of the      
          total 5254996 wavefronts). Check the L1 Wavefronts Shared Excessive table for the primary source locations.   
          The CUDA Best Practices Guide                                                                                 
           (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#shared-memory-in-matrix-multiplication-c
          -ab) has an example on optimizing shared memory accesses.                                                     

  void at::reduce_kernel<128, 4, at::ReduceOp<c10::Half, at::func_wrapper_t<c10::Half, at::sum_functor<c10::Half, float, c10::Half>::operator ()(at::TensorIterator &)::[lambda(float, float) (instance 1)]>, unsigned int, c10::Half, 4>>(T3) (2048, 1, 1)x(64, 2, 1), Context 1, Stream 7, Device 0, CC 8.9
    Section: GPU Speed Of Light Throughput
    ----------------------- ----------- ------------
    Metric Name             Metric Unit Metric Value
    ----------------------- ----------- ------------
    DRAM Frequency                  Ghz        11.19
    SM Frequency                    Ghz         2.28
    Elapsed Cycles                cycle        28043
    Memory Throughput                 %        48.47
    DRAM Throughput                   %        48.47
    Duration                         us        12.29
    L1/TEX Cache Throughput           %        23.48
    L2 Cache Throughput               %        34.93
    SM Active Cycles              cycle     24362.20
    Compute (SM) Throughput           %        26.26
    ----------------------- ----------- ------------

    OPT   This workload exhibits low compute throughput and memory bandwidth utilization relative to the peak           
          performance of this device. Achieved compute throughput and/or memory bandwidth below 60.0% of peak           
          typically indicate latency issues. Look at Scheduler Statistics and Warp State Statistics for potential       
          reasons.                                                                                                      

    Section: GPU Speed Of Light Roofline Chart
    INF   The ratio of peak float (fp32) to double (fp64) performance on this device is 64:1. The workload achieved     
          close to 1% of this device's fp32 peak performance and 0% of its fp64 peak performance. See the Kernel        
          Profiling Guide (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#roofline) for more details  
          on roofline analysis.                                                                                         

    Section: PM Sampling
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Maximum Buffer Size             Mbyte        25.17
    Dropped Samples                sample            0
    Maximum Sampling Interval          us            1
    # Pass Groups                                    2
    ------------------------- ----------- ------------

    Section: Compute Workload Analysis
    -------------------- ----------- ------------
    Metric Name          Metric Unit Metric Value
    -------------------- ----------- ------------
    Executed Ipc Active   inst/cycle         1.17
    Executed Ipc Elapsed  inst/cycle         1.02
    Issue Slots Busy               %        30.17
    Issued Ipc Active     inst/cycle         1.21
    SM Busy                        %        30.17
    -------------------- ----------- ------------

    INF   ALU is the highest-utilized pipeline (24.0%) based on active cycles, taking into account the rates of its     
          different instructions. It executes integer and logic operations. It is well-utilized, but should not be a    
          bottleneck.                                                                                                   

    Section: Memory Workload Analysis
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    Memory Throughput                Gbyte/s       347.03
    Mem Busy                               %        22.57
    Max Bandwidth                          %        48.47
    L1/TEX Hit Rate                        %        38.49
    L2 Compression Success Rate            %            0
    L2 Compression Ratio                                0
    L2 Compression Input Sectors      sector            0
    L2 Hit Rate                            %        61.77
    Mem Pipes Busy                         %         5.85
    ---------------------------- ----------- ------------

    Section: Memory Workload Analysis Tables
    OPT   Est. Speedup: 17.61%                                                                                          
          The memory access pattern for global stores to L1TEX might not be optimal. On average, only 8.0 of the 32     
          bytes transmitted per sector are utilized by each thread. This could possibly be caused by a stride between   
          threads. Check the Source Counters section for uncoalesced global stores.                                     

    Section: Scheduler Statistics
    ---------------------------- ----------- ------------
    Metric Name                  Metric Unit Metric Value
    ---------------------------- ----------- ------------
    One or More Eligible                   %        29.81
    Issued Warp Per Scheduler                        0.30
    No Eligible                            %        70.19
    Active Warps Per Scheduler          warp         9.04
    Eligible Warps Per Scheduler        warp         0.70
    ---------------------------- ----------- ------------

    OPT   Est. Local Speedup: 51.53%                                                                                    
          Every scheduler is capable of issuing one instruction per cycle, but for this workload each scheduler only    
          issues an instruction every 3.4 cycles. This might leave hardware resources underutilized and may lead to     
          less optimal performance. Out of the maximum of 12 warps per scheduler, this workload allocates an average    
          of 9.04 active warps per scheduler, but only an average of 0.70 warps were eligible per cycle. Eligible       
          warps are the subset of active warps that are ready to issue their next instruction. Every cycle with no      
          eligible warp results in no instruction being issued and the issue slot remains unused. To increase the       
          number of eligible warps, avoid possible load imbalances due to highly different execution durations per      
          warp. Reducing stalls indicated on the Warp State Statistics and Source Counters sections can help, too.      

    Section: Warp State Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Warp Cycles Per Issued Instruction             cycle        30.34
    Warp Cycles Per Executed Instruction           cycle        31.23
    Avg. Active Threads Per Warp                                   32
    Avg. Not Predicated Off Threads Per Warp                    29.71
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 41.49%                                                                                          
          On average, each warp of this workload spends 12.6 cycles being stalled waiting for a scoreboard dependency   
          on a L1TEX (local, global, surface, texture) operation. Find the instruction producing the data being waited  
          upon to identify the culprit. To reduce the number of cycles waiting on L1TEX data accesses verify the        
          memory access patterns are optimal for the target architecture, attempt to increase cache hit rates by        
          increasing data locality (coalescing), or by changing the cache configuration. Consider moving frequently     
          used data to shared memory. This stall type represents about 41.5% of the total average of 30.3 cycles        
          between issuing two instructions.                                                                             
    ----- --------------------------------------------------------------------------------------------------------------
    INF   Check the Warp Stall Sampling (All Samples) table for the top stall locations in your source based on         
          sampling data. The Kernel Profiling Guide                                                                     
          (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-reference) provides more details    
          on each stall reason.                                                                                         

    Section: Instruction Statistics
    ---------------------------------------- ----------- ------------
    Metric Name                              Metric Unit Metric Value
    ---------------------------------------- ----------- ------------
    Avg. Executed Instructions Per Scheduler        inst      7142.40
    Executed Instructions                           inst      2285568
    Avg. Issued Instructions Per Scheduler          inst      7351.14
    Issued Instructions                             inst      2352366
    ---------------------------------------- ----------- ------------

    OPT   Est. Speedup: 5.885%                                                                                          
          This kernel executes 0 fused and 163840 non-fused FP32 instructions. By converting pairs of non-fused         
          instructions to their fused (https://docs.nvidia.com/cuda/floating-point/#cuda-and-floating-point),           
          higher-throughput equivalent, the achieved FP32 performance could be increased by up to 50% (relative to its  
          current performance). Check the Source page to identify where this kernel executes FP32 instructions.         

    Section: Launch Statistics
    -------------------------------- --------------- ---------------
    Metric Name                          Metric Unit    Metric Value
    -------------------------------- --------------- ---------------
    Block Size                                                   128
    Function Cache Configuration                     CachePreferNone
    Grid Size                                                   2048
    Registers Per Thread             register/thread              44
    Shared Memory Configuration Size           Kbyte           32.77
    Driver Shared Memory Per Block       Kbyte/block            1.02
    Dynamic Shared Memory Per Block       byte/block               0
    Static Shared Memory Per Block        byte/block              16
    # SMs                                         SM              80
    Stack Size                                                  1024
    Threads                                   thread          262144
    # TPCs                                                        40
    Enabled TPC IDs                                              all
    Uses Green Context                                             0
    Waves Per SM                                                2.56
    -------------------------------- --------------- ---------------

    OPT   Est. Speedup: 33.33%                                                                                          
          A wave of thread blocks is defined as the maximum number of blocks that can be executed in parallel on the    
          target GPU. The number of blocks in a wave depends on the number of multiprocessors and the theoretical       
          occupancy of the kernel. This kernel launch results in 2 full waves and a partial wave of 449 thread blocks.  
          Under the assumption of a uniform execution duration of all thread blocks, this partial wave may account for  
          up to 33.3% of the total runtime of this kernel. Try launching a grid with no partial wave. The overall       
          impact of this tail effect also lessens with the number of full waves executed for a grid. See the Hardware   
          Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for     
          more details on launch configurations.                                                                        

    Section: Occupancy
    ------------------------------- ----------- ------------
    Metric Name                     Metric Unit Metric Value
    ------------------------------- ----------- ------------
    Block Limit SM                        block           24
    Block Limit Registers                 block           10
    Block Limit Shared Mem                block           28
    Block Limit Warps                     block           12
    Theoretical Active Warps per SM        warp           40
    Theoretical Occupancy                     %        83.33
    Achieved Occupancy                        %        74.99
    Achieved Active Warps Per SM           warp        36.00
    ------------------------------- ----------- ------------

    Section: GPU and Memory Workload Distribution
    -------------------------- ----------- ------------
    Metric Name                Metric Unit Metric Value
    -------------------------- ----------- ------------
    Average DRAM Active Cycles       cycle        66630
    Total DRAM Elapsed Cycles        cycle      1099776
    Average L1 Active Cycles         cycle     24362.20
    Total L1 Elapsed Cycles          cycle      2239552
    Average L2 Active Cycles         cycle     17878.34
    Total L2 Elapsed Cycles          cycle       762368
    Average SM Active Cycles         cycle     24362.20
    Total SM Elapsed Cycles          cycle      2239552
    Average SMSP Active Cycles       cycle     24659.95
    Total SMSP Elapsed Cycles        cycle      8958208
    -------------------------- ----------- ------------

    Section: Source Counters
    ------------------------- ----------- ------------
    Metric Name               Metric Unit Metric Value
    ------------------------- ----------- ------------
    Branch Instructions Ratio           %         0.16
    Branch Instructions              inst       376832
    Branch Efficiency                   %          100
    Avg. Divergent Branches                          0
    ------------------------- ----------- ------------

    OPT   Est. Speedup: 37.52%                                                                                          
          This kernel has uncoalesced global accesses resulting in a total of 196608 excessive sectors (50% of the      
          total 393216 sectors). Check the L2 Theoretical Sectors Global Excessive table for the primary source         
          locations. The CUDA Programming Guide                                                                         
          (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#device-memory-accesses) has additional      
          information on reducing uncoalesced device memory accesses.                                                   

